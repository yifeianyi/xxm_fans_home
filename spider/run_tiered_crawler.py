#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å±‚çˆ¬è™«ä¸»æ§è„šæœ¬
å®ç°çƒ­æ•°æ®ï¼ˆ7å¤©å†…ï¼‰æ¯å°æ—¶çˆ¬å–ï¼Œå†·æ•°æ®ï¼ˆè¶…è¿‡7å¤©ï¼‰æ¯å¤©3æ¬¡çˆ¬å–
æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘çˆ¬å–çƒ­æ•°æ®å’Œå†·æ•°æ®ï¼Œå®Œæˆåç»Ÿä¸€å¯¼å…¥

è·¯å¾„: spider/run_tiered_crawler.py

ç”¨æ³•:
    python spider/run_tiered_crawler.py --hot              # åªçˆ¬å–çƒ­æ•°æ®
    python spider/run_tiered_crawler.py --cold             # åªçˆ¬å–å†·æ•°æ®
    python spider/run_tiered_crawler.py --all              # çˆ¬å–å…¨éƒ¨æ•°æ®
    python spider/run_tiered_crawler.py --scheduled        # æ ¹æ®å½“å‰æ—¶é—´è‡ªåŠ¨é€‰æ‹©
    python spider/run_tiered_crawler.py --stats            # æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡
"""

import argparse
import sys
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Optional, Dict, Any, Tuple, List

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BACKEND_PATH = os.path.join(PROJECT_ROOT, 'repo', 'xxm_fans_backend')

# æ·»åŠ åç«¯ç›®å½•åˆ°è·¯å¾„ï¼ˆå¿…é¡»é¦–å…ˆæ·»åŠ ï¼‰
sys.path.insert(0, BACKEND_PATH)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'xxm_fans_home.settings')

import django
django.setup()

# Django æ—¶åŒºæ”¯æŒï¼ˆå¿…é¡»åœ¨ django.setup() ä¹‹åå¯¼å…¥ï¼‰
from django.utils import timezone

# å¯¼å…¥åˆ†å±‚å¯¼å‡ºæ¨¡å—
from tools.spider.export_tiered import TieredViewsExporter, WorkTier, DEFAULT_HOT_DAYS
from tools.spider.crawl_views import ViewsCrawler, VIEWS_FILE
from tools.spider.import_views import ViewsImporter
from tools.spider.utils.logger import setup_views_logger, get_project_root

logger = setup_views_logger("run_tiered_crawler")

# å†·æ•°æ®çˆ¬å–æ—¶æ®µï¼ˆ24å°æ—¶åˆ¶ï¼‰
COLD_CRAWL_HOURS = [0, 8, 16]  # 00:00, 08:00, 16:00

# çº¿ç¨‹é”ï¼Œç”¨äºæ—¥å¿—åŒæ­¥
log_lock = threading.Lock()


def get_current_hour() -> int:
    """è·å–å½“å‰å°æ—¶ï¼ˆä½¿ç”¨ Django æœ¬åœ°æ—¶åŒºï¼‰"""
    return timezone.localtime().hour


def should_crawl_cold_now() -> bool:
    """
    åˆ¤æ–­å½“å‰æ˜¯å¦åº”è¯¥çˆ¬å–å†·æ•°æ®
    å†·æ•°æ®æ¯å¤©åªåœ¨æŒ‡å®šæ—¶æ®µçˆ¬å–
    
    Returns:
        bool: æ˜¯å¦åº”è¯¥çˆ¬å–å†·æ•°æ®
    """
    current_hour = get_current_hour()
    return current_hour in COLD_CRAWL_HOURS


def export_and_crawl_tier(
    tier: WorkTier,
    force: bool = False,
    request_delay_min: float = 1.0,
    request_delay_max: float = 3.0,
    max_retries: int = 2
) -> Tuple[bool, Dict[str, Any], Optional[str]]:
    """
    æ‰§è¡ŒæŒ‡å®šåˆ†å±‚çš„å¯¼å‡ºå’Œçˆ¬å–æµç¨‹ï¼ˆä¸åŒ…å«å¯¼å…¥ï¼‰
    
    Args:
        tier: åˆ†å±‚ç±»å‹ (HOT/COLD)
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        request_delay_min: æœ€å°è¯·æ±‚å»¶è¿Ÿ
        request_delay_max: æœ€å¤§è¯·æ±‚å»¶è¿Ÿ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Tuple[bool, dict, Optional[str]]: (æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯, è¾“å‡ºæ–‡ä»¶è·¯å¾„)
    """
    result_info = {
        "tier": tier.value,
        "start_time": datetime.now().isoformat(),
        "steps": {}
    }
    output_path = None
    
    with log_lock:
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹æ‰§è¡Œ{tier.value.upper()}æ•°æ®çˆ¬å–æµç¨‹")
        logger.info("=" * 60)

    # 1. å¯¼å‡ºæ•°æ®
    with log_lock:
        logger.info(f"\n[1/2] å¯¼å‡º{tier.value.upper()}æ•°æ®...")
    exporter = TieredViewsExporter()
    
    if tier == WorkTier.HOT:
        success, filepath, info = exporter.export_hot()
    elif tier == WorkTier.COLD:
        success, filepath, info = exporter.export_cold()
    else:
        success, filepath, info = exporter.export_all()
    
    if not success:
        error_msg = info.get('error', 'å¯¼å‡ºå¤±è´¥')
        with log_lock:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {error_msg}")
        result_info["steps"]["export"] = {"success": False, "error": error_msg}
        return False, result_info, None
    
    export_count = info.get('total_count', 0)
    with log_lock:
        logger.info(f"âœ“ å¯¼å‡ºæˆåŠŸ: {export_count} æ¡è®°å½• -> {filepath}")
    result_info["steps"]["export"] = {"success": True, "count": export_count, "file": filepath}
    
    if export_count == 0:
        with log_lock:
            logger.info(f"æ²¡æœ‰{tier.value.upper()}æ•°æ®éœ€è¦çˆ¬å–")
        result_info["status"] = "skipped"
        result_info["end_time"] = datetime.now().isoformat()
        return True, result_info, None

    # 2. çˆ¬å–æ•°æ®
    with log_lock:
        logger.info(f"\n[2/2] çˆ¬å–Bç«™{tier.value.upper()}æ•°æ®...")
    
    # å¤‡ä»½åŸå§‹æ–‡ä»¶è·¯å¾„
    original_views_file = VIEWS_FILE
    
    try:
        # ä¸´æ—¶æ›¿æ¢ views.json ä¸ºå¯¼å‡ºçš„æ–‡ä»¶
        import tools.spider.crawl_views as crawl_module
        crawl_module.VIEWS_FILE = filepath
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œä¼ å…¥ tier å‚æ•°ä»¥åŒºåˆ†è¾“å‡ºæ–‡ä»¶å
        crawler = ViewsCrawler(
            request_delay_min=request_delay_min,
            request_delay_max=request_delay_max,
            max_retries=max_retries,
            tier=tier.value  # ä¼ å…¥åˆ†å±‚ç±»å‹ï¼Œç”¨äºç”Ÿæˆä¸åŒçš„æ–‡ä»¶å
        )
        
        output_path = crawler.crawl()
        with log_lock:
            logger.info(f"âœ“ çˆ¬å–å®Œæˆ: {output_path}")
        result_info["steps"]["crawl"] = {"success": True, "output": output_path}
        
    except Exception as e:
        with log_lock:
            logger.error(f"çˆ¬å–å¤±è´¥: {e}")
        result_info["steps"]["crawl"] = {"success": False, "error": str(e)}
        return False, result_info, None
    finally:
        # æ¢å¤åŸå§‹è·¯å¾„
        crawl_module.VIEWS_FILE = original_views_file

    result_info["status"] = "success"
    result_info["end_time"] = datetime.now().isoformat()
    
    with log_lock:
        logger.info("\n" + "=" * 60)
        logger.info(f"{tier.value.upper()}æ•°æ®çˆ¬å–æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
        logger.info("=" * 60)
    
    return True, result_info, output_path


def merge_crawl_results(
    output_files: Dict[WorkTier, Optional[str]],
    date_str: str,
    hour_str: str
) -> Optional[str]:
    """
    åˆå¹¶å¤šä¸ªåˆ†å±‚çš„çˆ¬å–ç»“æœæ–‡ä»¶ä¸ºä¸€ä¸ªæ–‡ä»¶
    
    Args:
        output_files: å„åˆ†å±‚çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        date_str: æ—¥æœŸå­—ç¬¦ä¸²
        hour_str: å°æ—¶å­—ç¬¦ä¸²
        
    Returns:
        Optional[str]: åˆå¹¶åçš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ— æ³•åˆå¹¶åˆ™è¿”å› None
    """
    import json
    
    merged_data = {
        "session_id": f"merged_{date_str.replace('-', '')}{hour_str}00",
        "crawl_time": datetime.now().isoformat(),
        "crawl_hour": hour_str,
        "total_count": 0,
        "success_count": 0,
        "fail_count": 0,
        "skip_count": 0,
        "duration_seconds": 0,
        "data": [],
        "source_tiers": []
    }
    
    valid_files = []
    for tier, output_path in output_files.items():
        if output_path and os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                merged_data["total_count"] += data.get("total_count", 0)
                merged_data["success_count"] += data.get("success_count", 0)
                merged_data["fail_count"] += data.get("fail_count", 0)
                merged_data["skip_count"] += data.get("skip_count", 0)
                merged_data["duration_seconds"] += data.get("duration_seconds", 0)
                
                # åˆå¹¶æ•°æ®
                merged_data["data"].extend(data.get("data", []))
                merged_data["source_tiers"].append(tier.value)
                valid_files.append(output_path)
                
                logger.info(f"âœ“ åˆå¹¶ {tier.value.upper()} æ•°æ®: {len(data.get('data', []))} æ¡")
                
            except Exception as e:
                logger.error(f"âœ— è¯»å– {tier.value} æ•°æ®å¤±è´¥: {e}")
    
    if not valid_files:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶å¯ä»¥åˆå¹¶")
        return None
    
    # ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶è·¯å¾„
    project_root = get_project_root()
    merged_dir = os.path.join(project_root, "data", "spider", "views", date_str[:4], date_str[5:7], date_str[8:10])
    os.makedirs(merged_dir, exist_ok=True)
    
    merged_filename = f"{date_str}-{hour_str}_views_data_merged.json"
    merged_path = os.path.join(merged_dir, merged_filename)
    
    # å†™å…¥åˆå¹¶åçš„æ–‡ä»¶
    with open(merged_path, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ“ åˆå¹¶å®Œæˆ: æ€»è®¡ {merged_data['total_count']} æ¡è®°å½• -> {merged_path}")
    
    return merged_path


def import_crawl_result(
    output_path: str,
    date_str: str,
    hour_str: str,
    force: bool = False
) -> bool:
    """
    å¯¼å…¥å•ä¸ªçˆ¬å–ç»“æœæ–‡ä»¶åˆ°æ•°æ®åº“
    
    Args:
        output_path: çˆ¬å–ç»“æœæ–‡ä»¶è·¯å¾„
        date_str: æ—¥æœŸå­—ç¬¦ä¸²
        hour_str: å°æ—¶å­—ç¬¦ä¸²
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    if not output_path or not os.path.exists(output_path):
        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¼å…¥: {output_path}")
        return False
    
    importer = ViewsImporter()
    
    try:
        importer.connect()
        
        # åŠ è½½æ•°æ®æ–‡ä»¶
        import json
        with open(output_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å¯¼å…¥æ•°æ®
        success = importer.import_data(data, force=force)
        importer.close()
        
        if success:
            logger.info(f"âœ“ å¯¼å…¥æˆåŠŸ: {output_path}")
        else:
            logger.error(f"å¯¼å…¥å¤±è´¥: {output_path}")
        
        return success
        
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {e}")
        return False


def run_parallel_crawl(
    tiers: List[WorkTier],
    force: bool = False,
    request_delay_min: float = 1.0,
    request_delay_max: float = 3.0,
    max_retries: int = 2
) -> Tuple[bool, Dict[str, Any], Dict[WorkTier, Optional[str]]]:
    """
    å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†å±‚çš„çˆ¬å–æµç¨‹
    
    Args:
        tiers: è¦çˆ¬å–çš„åˆ†å±‚åˆ—è¡¨
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        request_delay_min: æœ€å°è¯·æ±‚å»¶è¿Ÿ
        request_delay_max: æœ€å¤§è¯·æ±‚å»¶è¿Ÿ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Tuple[bool, dict, dict]: (æ•´ä½“æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯, å„åˆ†å±‚è¾“å‡ºæ–‡ä»¶è·¯å¾„)
    """
    results = {
        "start_time": timezone.localtime().strftime('%Y-%m-%d %H:%M:%S'),
        "tiers": {},
    }
    output_files: Dict[WorkTier, Optional[str]] = {}
    
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹å¹¶è¡Œçˆ¬å–: {[t.value for t in tiers]}")
    logger.info("=" * 60)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œçˆ¬å–
    with ThreadPoolExecutor(max_workers=len(tiers)) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_tier = {}
        for tier in tiers:
            future = executor.submit(
                export_and_crawl_tier,
                tier,
                force,
                request_delay_min,
                request_delay_max,
                max_retries
            )
            future_to_tier[future] = tier
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_tier):
            tier = future_to_tier[future]
            try:
                success, info, output_path = future.result()
                results["tiers"][tier.value] = info
                output_files[tier] = output_path
                
                if success:
                    logger.info(f"âœ“ {tier.value.upper()}æ•°æ®çˆ¬å–å®Œæˆ")
                else:
                    logger.error(f"âœ— {tier.value.upper()}æ•°æ®çˆ¬å–å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"âœ— {tier.value.upper()}æ•°æ®çˆ¬å–å¼‚å¸¸: {e}")
                results["tiers"][tier.value] = {"error": str(e)}
                output_files[tier] = None
    
    # åˆå¹¶å¹¶ç»Ÿä¸€å¯¼å…¥æ‰€æœ‰ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹åˆå¹¶å¹¶å¯¼å…¥æ•°æ®...")
    logger.info("=" * 60)
    
    crawl_time = datetime.now()
    date_str = crawl_time.strftime('%Y-%m-%d')
    hour_str = crawl_time.strftime('%H')
    
    # åˆå¹¶æ‰€æœ‰åˆ†å±‚çš„æ•°æ®æ–‡ä»¶
    merged_path = merge_crawl_results(output_files, date_str, hour_str)
    
    # ç»Ÿä¸€å¯¼å…¥åˆå¹¶åçš„æ–‡ä»¶
    all_import_success = True
    if merged_path:
        logger.info(f"\nå¯¼å…¥åˆå¹¶åçš„æ•°æ®...")
        success = import_crawl_result(merged_path, date_str, hour_str, force)
        # ä¸ºæ¯ä¸ªåˆ†å±‚è®°å½•å¯¼å…¥çŠ¶æ€
        for tier in output_files.keys():
            if tier.value in results["tiers"]:
                results["tiers"][tier.value]["import_success"] = success
        if not success:
            all_import_success = False
    else:
        logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦å¯¼å…¥")
        all_import_success = False
    
    results["end_time"] = timezone.localtime().strftime('%Y-%m-%d %H:%M:%S')
    
    # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªçˆ¬å–æˆåŠŸï¼Œä¸”å¯¼å…¥æˆåŠŸï¼‰
    any_crawl_success = any(
        info.get("status") == "success" 
        for info in results["tiers"].values()
    )
    overall_success = any_crawl_success and all_import_success
    
    logger.info("\n" + "=" * 60)
    logger.info("å¹¶è¡Œçˆ¬å–å’Œå¯¼å…¥æ‰§è¡Œå®Œæˆ")
    logger.info("=" * 60)
    
    return overall_success, results, output_files


def run_crawl_pipeline(
    tier: WorkTier, 
    views_file: str,
    force: bool = False,
    request_delay_min: float = 1.0,
    request_delay_max: float = 3.0,
    max_retries: int = 2
) -> Tuple[bool, Dict[str, Any]]:
    """
    æ‰§è¡ŒæŒ‡å®šåˆ†å±‚çš„å®Œæ•´çˆ¬å–æµç¨‹ï¼ˆä¸²è¡Œç‰ˆæœ¬ï¼Œç”¨äºå•ç‹¬æ‰§è¡Œï¼‰
    
    Args:
        tier: åˆ†å±‚ç±»å‹ (HOT/COLD)
        views_file: views.json æ–‡ä»¶è·¯å¾„ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        request_delay_min: æœ€å°è¯·æ±‚å»¶è¿Ÿ
        request_delay_max: æœ€å¤§è¯·æ±‚å»¶è¿Ÿ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Tuple[bool, dict]: (æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯)
    """
    # ä½¿ç”¨æ–°çš„å¹¶è¡Œå‡½æ•°ï¼Œä½†åªæ‰§è¡Œä¸€ä¸ª tier
    success, results, output_files = run_parallel_crawl(
        tiers=[tier],
        force=force,
        request_delay_min=request_delay_min,
        request_delay_max=request_delay_max,
        max_retries=max_retries
    )
    
    return success, results.get("tiers", {}).get(tier.value, {})


def run_scheduled_crawl(force: bool = False) -> Tuple[bool, Dict[str, Any]]:
    """
    æ ¹æ®å½“å‰æ—¶é—´æ‰§è¡Œè°ƒåº¦çˆ¬å–
    - æ¯å°æ—¶éƒ½çˆ¬å–çƒ­æ•°æ®
    - åªåœ¨æŒ‡å®šæ—¶æ®µçˆ¬å–å†·æ•°æ®
    - çƒ­æ•°æ®å’Œå†·æ•°æ®å¹¶å‘æ‰§è¡Œï¼Œå®Œæˆåç»Ÿä¸€å¯¼å…¥
    
    Args:
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        
    Returns:
        Tuple[bool, dict]: (æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯)
    """
    current_hour = get_current_hour()
    current_time = timezone.localtime().strftime('%Y-%m-%d %H:%M:%S')
    
    logger.info("=" * 60)
    logger.info(f"æ‰§è¡Œè°ƒåº¦çˆ¬å– - å½“å‰æ—¶é—´: {current_time}")
    logger.info(f"å½“å‰æ—¶æ®µ: {current_hour}:00")
    logger.info("=" * 60)
    
    # ç¡®å®šè¦çˆ¬å–çš„åˆ†å±‚
    tiers_to_crawl = [WorkTier.HOT]  # çƒ­æ•°æ®å§‹ç»ˆçˆ¬å–
    
    if current_hour in COLD_CRAWL_HOURS:
        tiers_to_crawl.append(WorkTier.COLD)
        logger.info(f"\næœ¬æ—¶æ®µå°†çˆ¬å–: çƒ­æ•°æ® + å†·æ•°æ®ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰")
    else:
        next_cold_hours = [h for h in COLD_CRAWL_HOURS if h > current_hour]
        next_cold = next_cold_hours[0] if next_cold_hours else COLD_CRAWL_HOURS[0]
        logger.info(f"\næœ¬æ—¶æ®µå°†çˆ¬å–: ä»…çƒ­æ•°æ®")
        logger.info(f"ä¸‹æ¬¡å†·æ•°æ®çˆ¬å–æ—¶é—´: {next_cold}:00")
    
    # æ‰§è¡Œå¹¶è¡Œçˆ¬å–å’Œç»Ÿä¸€å¯¼å…¥
    success, results, output_files = run_parallel_crawl(tiers=tiers_to_crawl, force=force)
    
    return success, results


def main():
    parser = argparse.ArgumentParser(
        description='åˆ†å±‚çˆ¬è™«ä¸»æ§è„šæœ¬ - çƒ­æ•°æ®æ¯å°æ—¶çˆ¬å–ï¼Œå†·æ•°æ®æ¯å¤©3æ¬¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åªçˆ¬å–çƒ­æ•°æ®ï¼ˆ7å¤©å†…å‘å¸ƒçš„ä½œå“ï¼‰- æ¯å°æ—¶æ‰§è¡Œ
  python run_tiered_crawler.py --hot
  
  # åªçˆ¬å–å†·æ•°æ®ï¼ˆ7å¤©å‰å‘å¸ƒçš„ä½œå“ï¼‰- æ¯å¤©0/8/16ç‚¹æ‰§è¡Œ
  python run_tiered_crawler.py --cold
  
  # çˆ¬å–å…¨éƒ¨æ•°æ®ï¼ˆçƒ­+å†·ï¼Œå¹¶å‘æ‰§è¡Œï¼‰
  python run_tiered_crawler.py --all
  
  # æ ¹æ®å½“å‰æ—¶é—´è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰
  python run_tiered_crawler.py --scheduled
  
  # æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡ä¿¡æ¯
  python run_tiered_crawler.py --stats
  
  # å¼ºåˆ¶é‡æ–°å¯¼å…¥ï¼ˆå³ä½¿æ•°æ®å·²å­˜åœ¨ï¼‰
  python run_tiered_crawler.py --hot --force
  
  # è°ƒæ•´è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
  python run_tiered_crawler.py --hot --delay-min 0.5 --delay-max 1.5
        """
    )
    
    # æ‰§è¡Œæ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--hot', action='store_true', help='åªçˆ¬å–çƒ­æ•°æ®ï¼ˆ7å¤©å†…ï¼‰')
    mode_group.add_argument('--cold', action='store_true', help='åªçˆ¬å–å†·æ•°æ®ï¼ˆ7å¤©å‰ï¼‰')
    mode_group.add_argument('--all', action='store_true', help='çˆ¬å–å…¨éƒ¨æ•°æ®')
    mode_group.add_argument('--scheduled', action='store_true', help='æ ¹æ®æ—¶é—´è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰')
    mode_group.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡ä¿¡æ¯')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¯¼å…¥')
    parser.add_argument('--delay-min', type=float, default=1.0, help='æœ€å°è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--delay-max', type=float, default=3.0, help='æœ€å¤§è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--retries', type=int, default=2, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        exporter = TieredViewsExporter()
        stats = exporter.get_tier_stats()
        
        print("\n" + "=" * 70)
        print("åˆ†å±‚çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 70)
        print(f"\nğŸ“Š çƒ­æ•°æ®é˜ˆå€¼: {stats['hot_days_threshold']} å¤©")
        print(f"ğŸ“… åˆ†å±‚æˆªæ­¢æ—¶é—´: {stats['cutoff_date'][:10]}")
        print(f"ğŸ“ æ€»ä½œå“æ•°: {stats['total_works']}")
        
        print(f"\nğŸ”¥ çƒ­æ•°æ®ï¼ˆæœ€è¿‘{stats['hot_days_threshold']}å¤©ï¼‰: {stats['hot_works']['count']} æ¡")
        print(f"   çˆ¬å–é¢‘ç‡: æ¯å°æ—¶")
        if stats['hot_works']['newest']:
            print(f"   æœ€æ–°: {stats['hot_works']['newest']['title'][:45]}...")
        if stats['hot_works']['oldest']:
            print(f"   æœ€æ—§: {stats['hot_works']['oldest']['title'][:45]}...")
        
        print(f"\nâ„ï¸ å†·æ•°æ®ï¼ˆ{stats['hot_days_threshold']}å¤©å‰ï¼‰: {stats['cold_works']['count']} æ¡")
        print(f"   çˆ¬å–é¢‘ç‡: æ¯å¤©3æ¬¡ (00:00, 08:00, 16:00)")
        if stats['cold_works']['newest']:
            print(f"   æœ€æ–°: {stats['cold_works']['newest']['title'][:45]}...")
        if stats['cold_works']['oldest']:
            print(f"   æœ€æ—§: {stats['cold_works']['oldest']['title'][:45]}...")
        
        print("\n" + "=" * 70)
        print("ğŸ’¡ å½“å‰æ—¶æ®µçˆ¬å–ç­–ç•¥:")
        current_hour = get_current_hour()
        print(f"   å½“å‰æ—¶é—´: {current_hour}:00")
        print(f"   çƒ­æ•°æ®: å§‹ç»ˆçˆ¬å–")
        if should_crawl_cold_now():
            print(f"   å†·æ•°æ®: âœ… æœ¬æ—¶æ®µæ‰§è¡Œçˆ¬å–ï¼ˆä¸çƒ­æ•°æ®å¹¶å‘ï¼‰")
        else:
            next_cold = None
            for h in COLD_CRAWL_HOURS:
                if h > current_hour:
                    next_cold = h
                    break
            if next_cold is None:
                next_cold = COLD_CRAWL_HOURS[0]
            print(f"   å†·æ•°æ®: â¸ï¸ è·³è¿‡ï¼ˆä¸‹æ¬¡: {next_cold}:00ï¼‰")
        print("=" * 70 + "\n")
        
        sys.exit(0)
    
    # æ‰§è¡Œçˆ¬å–
    success = False
    
    try:
        if args.hot:
            success, info = run_crawl_pipeline(
                tier=WorkTier.HOT,
                views_file="views_hot.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
        elif args.cold:
            success, info = run_crawl_pipeline(
                tier=WorkTier.COLD,
                views_file="views_cold.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
        elif args.all:
            # å¹¶è¡Œçˆ¬å–çƒ­æ•°æ®å’Œå†·æ•°æ®
            success, results, output_files = run_parallel_crawl(
                tiers=[WorkTier.HOT, WorkTier.COLD],
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
        elif args.scheduled:
            success, info = run_scheduled_crawl(force=args.force)
        else:
            # é»˜è®¤æ‰§è¡Œè°ƒåº¦æ¨¡å¼
            print("ä½¿ç”¨é»˜è®¤æ¨¡å¼ï¼š--scheduledï¼ˆä½¿ç”¨ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹ï¼‰")
            success, info = run_scheduled_crawl(force=args.force)
            
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­çˆ¬å–ä»»åŠ¡")
        sys.exit(130)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        success = False
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
