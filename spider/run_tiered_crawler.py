#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å±‚çˆ¬è™«ä¸»æ§è„šæœ¬
å®ç°çƒ­æ•°æ®ï¼ˆ7å¤©å†…ï¼‰æ¯å°æ—¶çˆ¬å–ï¼Œå†·æ•°æ®ï¼ˆè¶…è¿‡7å¤©ï¼‰æ¯å¤©3æ¬¡çˆ¬å–

è·¯å¾„: spider/run_tiered_crawler.py

ç”¨æ³•:
    python spider/run_tiered_crawler.py --hot              # åªçˆ¬å–çƒ­æ•°æ®
    python spider/run_tiered_crawler.py --cold             # åªçˆ¬å–å†·æ•°æ®
    python spider/run_tiered_crawler.py --all              # çˆ¬å–å…¨éƒ¨æ•°æ®
    python spider/run_tiered_crawler.py --scheduled        # æ ¹æ®å½“å‰æ—¶é—´è‡ªåŠ¨é€‰æ‹©
    python spider/run_tiered_crawler.py --stats            # æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡
"""

import argparse
import sys
import os
from datetime import datetime
from typing import Optional, Dict, Any, Tuple

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BACKEND_PATH = os.path.join(PROJECT_ROOT, 'repo', 'xxm_fans_backend')

# æ·»åŠ åç«¯ç›®å½•åˆ°è·¯å¾„ï¼ˆå¿…é¡»é¦–å…ˆæ·»åŠ ï¼‰
sys.path.insert(0, BACKEND_PATH)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'xxm_fans_home.settings')

import django
django.setup()

# å¯¼å…¥åˆ†å±‚å¯¼å‡ºæ¨¡å—
from tools.spider.export_tiered import TieredViewsExporter, WorkTier, DEFAULT_HOT_DAYS
from tools.spider.crawl_views import ViewsCrawler, VIEWS_FILE
from tools.spider.import_views import ViewsImporter
from tools.spider.utils.logger import setup_views_logger, get_project_root

logger = setup_views_logger("run_tiered_crawler")

# å†·æ•°æ®çˆ¬å–æ—¶æ®µï¼ˆ24å°æ—¶åˆ¶ï¼‰
COLD_CRAWL_HOURS = [0, 8, 16]  # 00:00, 08:00, 16:00


def get_current_hour() -> int:
    """è·å–å½“å‰å°æ—¶"""
    return datetime.now().hour


def should_crawl_cold_now() -> bool:
    """
    åˆ¤æ–­å½“å‰æ˜¯å¦åº”è¯¥çˆ¬å–å†·æ•°æ®
    å†·æ•°æ®æ¯å¤©åªåœ¨æŒ‡å®šæ—¶æ®µçˆ¬å–
    
    Returns:
        bool: æ˜¯å¦åº”è¯¥çˆ¬å–å†·æ•°æ®
    """
    current_hour = get_current_hour()
    return current_hour in COLD_CRAWL_HOURS


def run_crawl_pipeline(
    tier: WorkTier, 
    views_file: str,
    force: bool = False,
    request_delay_min: float = 1.0,
    request_delay_max: float = 3.0,
    max_retries: int = 2
) -> Tuple[bool, Dict[str, Any]]:
    """
    æ‰§è¡ŒæŒ‡å®šåˆ†å±‚çš„å®Œæ•´çˆ¬å–æµç¨‹
    
    Args:
        tier: åˆ†å±‚ç±»å‹ (HOT/COLD)
        views_file: views.json æ–‡ä»¶è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        request_delay_min: æœ€å°è¯·æ±‚å»¶è¿Ÿ
        request_delay_max: æœ€å¤§è¯·æ±‚å»¶è¿Ÿ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Tuple[bool, dict]: (æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯)
    """
    result_info = {
        "tier": tier.value,
        "start_time": datetime.now().isoformat(),
        "steps": {}
    }
    
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹æ‰§è¡Œ{tier.value.upper()}æ•°æ®çˆ¬å–æµç¨‹")
    logger.info("=" * 60)

    # 1. å¯¼å‡ºæ•°æ®
    logger.info(f"\n[1/3] å¯¼å‡º{tier.value.upper()}æ•°æ®...")
    exporter = TieredViewsExporter()
    
    if tier == WorkTier.HOT:
        success, filepath, info = exporter.export_hot()
    elif tier == WorkTier.COLD:
        success, filepath, info = exporter.export_cold()
    else:
        success, filepath, info = exporter.export_all()
    
    if not success:
        error_msg = info.get('error', 'å¯¼å‡ºå¤±è´¥')
        logger.error(f"å¯¼å‡ºå¤±è´¥: {error_msg}")
        result_info["steps"]["export"] = {"success": False, "error": error_msg}
        return False, result_info
    
    export_count = info.get('total_count', 0)
    logger.info(f"âœ“ å¯¼å‡ºæˆåŠŸ: {export_count} æ¡è®°å½• -> {filepath}")
    result_info["steps"]["export"] = {"success": True, "count": export_count, "file": filepath}
    
    if export_count == 0:
        logger.info(f"æ²¡æœ‰{tier.value.upper()}æ•°æ®éœ€è¦çˆ¬å–ï¼Œæµç¨‹ç»“æŸ")
        result_info["status"] = "skipped"
        return True, result_info

    # 2. çˆ¬å–æ•°æ®ï¼ˆä¸´æ—¶ä¿®æ”¹ VIEWS_FILE æŒ‡å‘å¯¼å‡ºçš„æ–‡ä»¶ï¼‰
    logger.info(f"\n[2/3] çˆ¬å–Bç«™æ•°æ®...")
    
    # å¤‡ä»½åŸå§‹æ–‡ä»¶è·¯å¾„
    original_views_file = VIEWS_FILE
    
    try:
        # ä¸´æ—¶æ›¿æ¢ views.json ä¸ºå¯¼å‡ºçš„æ–‡ä»¶
        import tools.spider.crawl_views as crawl_module
        crawl_module.VIEWS_FILE = filepath
        
        crawler = ViewsCrawler(
            request_delay_min=request_delay_min,
            request_delay_max=request_delay_max,
            max_retries=max_retries
        )
        
        output_path = crawler.crawl()
        logger.info(f"âœ“ çˆ¬å–å®Œæˆ: {output_path}")
        result_info["steps"]["crawl"] = {"success": True, "output": output_path}
        
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥: {e}")
        result_info["steps"]["crawl"] = {"success": False, "error": str(e)}
        return False, result_info
    finally:
        # æ¢å¤åŸå§‹è·¯å¾„
        crawl_module.VIEWS_FILE = original_views_file

    # 3. å¯¼å…¥æ•°æ®
    logger.info(f"\n[3/3] å¯¼å…¥æ•°æ®åˆ°SQLite...")
    importer = ViewsImporter()
    
    try:
        importer.connect()
        
        # ä»çˆ¬å–ç»“æœæ–‡ä»¶ä¸­è·å–æ—¥æœŸå’Œå°æ—¶
        crawl_time = datetime.now()
        date_str = crawl_time.strftime('%Y-%m-%d')
        hour_str = crawl_time.strftime('%H')
        
        success = importer.import_by_date(date_str, hour_str, auto_find=False, force=force)
        importer.close()
        
        if success:
            logger.info("âœ“ å¯¼å…¥æˆåŠŸ")
            result_info["steps"]["import"] = {"success": True}
        else:
            logger.error("å¯¼å…¥å¤±è´¥")
            result_info["steps"]["import"] = {"success": False, "error": "å¯¼å…¥å¤±è´¥"}
            return False, result_info
            
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {e}")
        result_info["steps"]["import"] = {"success": False, "error": str(e)}
        return False, result_info

    result_info["status"] = "success"
    result_info["end_time"] = datetime.now().isoformat()
    
    logger.info("\n" + "=" * 60)
    logger.info(f"{tier.value.upper()}æ•°æ®çˆ¬å–æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
    logger.info("=" * 60)
    
    return True, result_info


def run_scheduled_crawl(force: bool = False) -> Tuple[bool, Dict[str, Any]]:
    """
    æ ¹æ®å½“å‰æ—¶é—´æ‰§è¡Œè°ƒåº¦çˆ¬å–
    - æ¯å°æ—¶éƒ½çˆ¬å–çƒ­æ•°æ®
    - åªåœ¨æŒ‡å®šæ—¶æ®µçˆ¬å–å†·æ•°æ®
    
    Args:
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
        
    Returns:
        Tuple[bool, dict]: (æ˜¯å¦æˆåŠŸ, æ‰§è¡Œä¿¡æ¯)
    """
    current_hour = get_current_hour()
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    logger.info("=" * 60)
    logger.info(f"æ‰§è¡Œè°ƒåº¦çˆ¬å– - å½“å‰æ—¶é—´: {current_time}")
    logger.info(f"å½“å‰æ—¶æ®µ: {current_hour}:00")
    logger.info("=" * 60)
    
    results = {
        "scheduled_time": current_time,
        "current_hour": current_hour,
        "hot": None,
        "cold": None,
    }
    
    # 1. å§‹ç»ˆçˆ¬å–çƒ­æ•°æ®ï¼ˆæ¯å°æ—¶ï¼‰
    logger.info("\nã€é˜¶æ®µ1ã€‘çˆ¬å–çƒ­æ•°æ®ï¼ˆæ¯å°æ—¶æ‰§è¡Œï¼‰")
    hot_success, hot_info = run_crawl_pipeline(
        tier=WorkTier.HOT,
        views_file="views_hot.json",
        force=force
    )
    results["hot"] = hot_info
    
    if not hot_success:
        logger.warning("çƒ­æ•°æ®çˆ¬å–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œå†·æ•°æ®æ£€æŸ¥...")
    
    # 2. åªåœ¨æŒ‡å®šæ—¶æ®µçˆ¬å–å†·æ•°æ®
    if should_crawl_cold_now():
        logger.info(f"\nã€é˜¶æ®µ2ã€‘çˆ¬å–å†·æ•°æ®ï¼ˆ{current_hour}:00 æ—¶æ®µæ‰§è¡Œï¼‰")
        cold_success, cold_info = run_crawl_pipeline(
            tier=WorkTier.COLD,
            views_file="views_cold.json",
            force=force
        )
        results["cold"] = cold_info
        
        if not cold_success:
            logger.error("å†·æ•°æ®çˆ¬å–å¤±è´¥")
    else:
        next_cold_hours = [h for h in COLD_CRAWL_HOURS if h > current_hour]
        if next_cold_hours:
            next_cold = next_cold_hours[0]
        else:
            next_cold = COLD_CRAWL_HOURS[0]
        logger.info(f"\nã€é˜¶æ®µ2ã€‘è·³è¿‡å†·æ•°æ®çˆ¬å–ï¼ˆä¸åœ¨çˆ¬å–æ—¶æ®µï¼‰")
        logger.info(f"        ä¸‹æ¬¡çˆ¬å–æ—¶é—´: {next_cold}:00")
        results["cold"] = {"skipped": True, "next_scheduled": f"{next_cold}:00"}
    
    # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
    overall_success = results["hot"] and results["hot"].get("status") == "success"
    
    logger.info("\n" + "=" * 60)
    logger.info("è°ƒåº¦çˆ¬å–æ‰§è¡Œå®Œæˆ")
    logger.info("=" * 60)
    
    return overall_success, results


def main():
    parser = argparse.ArgumentParser(
        description='åˆ†å±‚çˆ¬è™«ä¸»æ§è„šæœ¬ - çƒ­æ•°æ®æ¯å°æ—¶çˆ¬å–ï¼Œå†·æ•°æ®æ¯å¤©3æ¬¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åªçˆ¬å–çƒ­æ•°æ®ï¼ˆ7å¤©å†…å‘å¸ƒçš„ä½œå“ï¼‰- æ¯å°æ—¶æ‰§è¡Œ
  python run_tiered_crawler.py --hot
  
  # åªçˆ¬å–å†·æ•°æ®ï¼ˆ7å¤©å‰å‘å¸ƒçš„ä½œå“ï¼‰- æ¯å¤©0/8/16ç‚¹æ‰§è¡Œ
  python run_tiered_crawler.py --cold
  
  # çˆ¬å–å…¨éƒ¨æ•°æ®ï¼ˆçƒ­+å†·ï¼‰
  python run_tiered_crawler.py --all
  
  # æ ¹æ®å½“å‰æ—¶é—´è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰
  python run_tiered_crawler.py --scheduled
  
  # æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡ä¿¡æ¯
  python run_tiered_crawler.py --stats
  
  # å¼ºåˆ¶é‡æ–°å¯¼å…¥ï¼ˆå³ä½¿æ•°æ®å·²å­˜åœ¨ï¼‰
  python run_tiered_crawler.py --hot --force
  
  # è°ƒæ•´è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
  python run_tiered_crawler.py --hot --delay-min 0.5 --delay-max 1.5
        """
    )
    
    # æ‰§è¡Œæ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--hot', action='store_true', help='åªçˆ¬å–çƒ­æ•°æ®ï¼ˆ7å¤©å†…ï¼‰')
    mode_group.add_argument('--cold', action='store_true', help='åªçˆ¬å–å†·æ•°æ®ï¼ˆ7å¤©å‰ï¼‰')
    mode_group.add_argument('--all', action='store_true', help='çˆ¬å–å…¨éƒ¨æ•°æ®')
    mode_group.add_argument('--scheduled', action='store_true', help='æ ¹æ®æ—¶é—´è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰')
    mode_group.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡ä¿¡æ¯')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¯¼å…¥')
    parser.add_argument('--delay-min', type=float, default=1.0, help='æœ€å°è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--delay-max', type=float, default=3.0, help='æœ€å¤§è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--retries', type=int, default=2, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        exporter = TieredViewsExporter()
        stats = exporter.get_tier_stats()
        
        print("\n" + "=" * 70)
        print("åˆ†å±‚çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 70)
        print(f"\nğŸ“Š çƒ­æ•°æ®é˜ˆå€¼: {stats['hot_days_threshold']} å¤©")
        print(f"ğŸ“… åˆ†å±‚æˆªæ­¢æ—¶é—´: {stats['cutoff_date'][:10]}")
        print(f"ğŸ“ æ€»ä½œå“æ•°: {stats['total_works']}")
        
        print(f"\nğŸ”¥ çƒ­æ•°æ®ï¼ˆæœ€è¿‘{stats['hot_days_threshold']}å¤©ï¼‰: {stats['hot_works']['count']} æ¡")
        print(f"   çˆ¬å–é¢‘ç‡: æ¯å°æ—¶")
        if stats['hot_works']['newest']:
            print(f"   æœ€æ–°: {stats['hot_works']['newest']['title'][:45]}...")
        if stats['hot_works']['oldest']:
            print(f"   æœ€æ—§: {stats['hot_works']['oldest']['title'][:45]}...")
        
        print(f"\nâ„ï¸ å†·æ•°æ®ï¼ˆ{stats['hot_days_threshold']}å¤©å‰ï¼‰: {stats['cold_works']['count']} æ¡")
        print(f"   çˆ¬å–é¢‘ç‡: æ¯å¤©3æ¬¡ (00:00, 08:00, 16:00)")
        if stats['cold_works']['newest']:
            print(f"   æœ€æ–°: {stats['cold_works']['newest']['title'][:45]}...")
        if stats['cold_works']['oldest']:
            print(f"   æœ€æ—§: {stats['cold_works']['oldest']['title'][:45]}...")
        
        print("\n" + "=" * 70)
        print("ğŸ’¡ å½“å‰æ—¶æ®µçˆ¬å–ç­–ç•¥:")
        current_hour = get_current_hour()
        print(f"   å½“å‰æ—¶é—´: {current_hour}:00")
        print(f"   çƒ­æ•°æ®: å§‹ç»ˆçˆ¬å–")
        if should_crawl_cold_now():
            print(f"   å†·æ•°æ®: âœ… æœ¬æ—¶æ®µæ‰§è¡Œçˆ¬å–")
        else:
            next_cold = None
            for h in COLD_CRAWL_HOURS:
                if h > current_hour:
                    next_cold = h
                    break
            if next_cold is None:
                next_cold = COLD_CRAWL_HOURS[0]
            print(f"   å†·æ•°æ®: â¸ï¸ è·³è¿‡ï¼ˆä¸‹æ¬¡: {next_cold}:00ï¼‰")
        print("=" * 70 + "\n")
        
        sys.exit(0)
    
    # æ‰§è¡Œçˆ¬å–
    success = False
    
    try:
        if args.hot:
            success, info = run_crawl_pipeline(
                tier=WorkTier.HOT,
                views_file="views_hot.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
        elif args.cold:
            success, info = run_crawl_pipeline(
                tier=WorkTier.COLD,
                views_file="views_cold.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
        elif args.all:
            # å…ˆçˆ¬çƒ­æ•°æ®ï¼Œå†çˆ¬å†·æ•°æ®
            hot_success, hot_info = run_crawl_pipeline(
                tier=WorkTier.HOT,
                views_file="views_hot.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
            cold_success, cold_info = run_crawl_pipeline(
                tier=WorkTier.COLD,
                views_file="views_cold.json",
                force=args.force,
                request_delay_min=args.delay_min,
                request_delay_max=args.delay_max,
                max_retries=args.retries
            )
            success = hot_success and cold_success
        elif args.scheduled:
            success, info = run_scheduled_crawl(force=args.force)
        else:
            # é»˜è®¤æ‰§è¡Œè°ƒåº¦æ¨¡å¼
            print("ä½¿ç”¨é»˜è®¤æ¨¡å¼ï¼š--scheduledï¼ˆä½¿ç”¨ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹ï¼‰")
            success, info = run_scheduled_crawl(force=args.force)
            
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­çˆ¬å–ä»»åŠ¡")
        sys.exit(130)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        success = False
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
