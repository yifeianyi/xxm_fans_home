# 粉丝数数据分析数据流说明

## 一、数据流总览

```
┌─────────────────┐
│  B站API        │
│  (每小时调用)   │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  爬虫脚本       │
│  spider/        │
│  get_bilibili_  │
│  fans_count.py  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  JSON文件       │
│  data/spider/   │
│  fans_count/    │
│  */*/b_fans_    │
│  count_*.json   │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  【自动】导入    │
│  tools/         │
│  ingest_follower│
│  _data.py       │
│  (定时任务自动)  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Django数据库   │
│  - Account      │
│  - Follower     │
│    Metrics      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  REST API       │
│  /api/data-     │
│  analytics/     │
│  followers/...  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  前端页面       │
│  DataAnalysis   │
│  Page           │
└─────────────────┘
```

## 二、详细数据流程

### 阶段 1：数据采集

**触发**：每小时定时任务（cron）

**输入**：
- B站账号 UID 列表（咻咻满: 37754047，咻小满: 480116537）

**执行**：
```bash
# 爬虫脚本
spider/get_bilibili_fans_count.py
```

**输出**：
- JSON 文件路径：`data/spider/fans_count/{year}/{month}/b_fans_count_{timestamp}.json`
- 示例：`data/spider/fans_count/2026/02/b_fans_count_2026-02-01-12.json`

**数据格式**：
```json
{
  "update_time": "2026-02-01 12:00:00",
  "accounts": [
    {
      "uid": "37754047",
      "name": "咻咻满",
      "follower": 1254800,
      "status": "success",
      "timestamp": "2026-02-01 12:00:00"
    },
    {
      "uid": "480116537",
      "name": "咻小满",
      "follower": 158000,
      "status": "success",
      "timestamp": "2026-02-01 12:00:00"
    }
  ]
}
```

---

### 阶段 2：数据导入

**触发**：
- **自动化**：定时任务自动触发（每小时爬虫成功后）
- **手动**：执行导入脚本（用于导入历史数据）

**自动化流程**：
```bash
# 由 scripts/bilibili_fans_count_cron.sh 自动执行
# 步骤：
# 1. 找到最新生成的 JSON 文件
# 2. 调用导入脚本
# 3. 记录导入结果到日志
```

**手动执行**（仅用于历史数据）：
```bash
# 导入所有历史数据
cd /home/yifeianyi/Desktop/xxm_fans_home/repo/xxm_fans_backend
python tools/ingest_follower_data.py

# 导入单个文件
python tools/ingest_follower_data.py --file data/spider/fans_count/2026/02/b_fans_count_2026-02-01-12.json
```

**处理逻辑**：
1. 读取 JSON 文件
2. 遍历 `accounts` 数组
3. 对每个账号：
   - 查找或创建 `Account` 记录（根据 UID）
   - 创建 `FollowerMetrics` 记录（存储粉丝数和时间戳）

**数据库变更**：
```
Account 表：
┌────┬──────────┬────────┬──────────┬───────────┐
│ id │ uid      │ name   │ platform │ is_active │
├────┼──────────┼────────┼──────────┼───────────┤
│ 1  │ 37754047 │ 咻咻满 │ bilibili │ true      │
│ 2  │480116537 │ 咻小满 │ bilibili │ true      │
└────┴──────────┴────────┴──────────┴───────────┘

FollowerMetrics 表：
┌────┬──────────┬─────────────┬────────────────────┬────────────────────┐
│ id │ account  │follower_count│    crawl_time      │   ingest_time      │
├────┼──────────┼─────────────┼────────────────────┼────────────────────┤
│ 1  │    1     │   1254800   │2026-02-01 12:00:00 │2026-02-01 12:05:00 │
│ 2  │    2     │    158000   │2026-02-01 12:00:00 │2026-02-01 12:05:00 │
│ 3  │    1     │   1255200   │2026-02-01 13:00:00 │2026-02-01 13:05:00 │
│ 4  │    2     │    158050   │2026-02-01 13:00:00 │2026-02-01 13:05:00 │
└────┴──────────┴─────────────┴────────────────────┴────────────────────┘
```

**唯一性约束**：
- `(account_id, crawl_time)` 组合必须唯一
- 重复数据会被覆盖（`update_or_create`）

---

### 阶段 3：数据查询与聚合

**触发**：前端页面加载或用户交互

**API 端点**：
```
GET /api/data-analytics/followers/accounts/data/?granularity=DAY&days=30
```

**查询参数**：
- `granularity`：时间粒度（`HOUR`、`DAY`、`MONTH`）
- `days`：查询天数（默认 30）

**Service 层处理**：

1. **获取账号列表**
   ```python
   accounts = Account.objects.filter(is_active=True)
   ```

2. **计算时间范围**
   ```python
   # granularity = DAY, days = 30
   start_time = now - 30 days
   end_time = now
   ```

3. **查询并聚合数据**
   ```python
   # 使用 Django 的 Trunc 函数按时间粒度聚合
   FollowerMetrics.objects.filter(
       account=account,
       crawl_time__gte=start_time,
       crawl_time__lte=end_time
   ).annotate(
       time_bucket=TruncDay('crawl_time')
   ).values('time_bucket').annotate(
       follower_count=F('follower_count')
   ).order_by('time_bucket')
   ```

4. **计算增量**
   ```python
   # 当前值 - 前一个值 = 增量
   delta = current_value - previous_value
   ```

**返回数据格式**：
```json
{
  "code": 200,
  "message": "操作成功",
  "data": [
    {
      "id": "1",
      "name": "咻咻满",
      "totalFollowers": 1255200,
      "history": {
        "DAY": [
          {
            "time": "01-01",
            "value": 1250000,
            "delta": 0
          },
          {
            "time": "01-02",
            "value": 1250500,
            "delta": 500
          },
          {
            "time": "01-03",
            "value": 1251200,
            "delta": 700
          }
        ]
      }
    },
    {
      "id": "2",
      "name": "咻小满",
      "totalFollowers": 158050,
      "history": {
        "DAY": [
          {
            "time": "01-01",
            "value": 150000,
            "delta": 0
          },
          {
            "time": "01-02",
            "value": 150200,
            "delta": 200
          },
          {
            "time": "01-03",
            "value": 150500,
            "delta": 300
          }
        ]
      }
    }
  ]
}
```

---

### 阶段 4：前端展示

**触发**：React 组件渲染

**组件流程**：

1. **初始化**
   ```typescript
   useEffect(() => {
     const fetchData = async () => {
       const accounts = await songService.getAccountsWithGranularity('DAY');
       setAccounts(accounts);
     };
     fetchData();
   }, [granularity]);
   ```

2. **API 调用**
   ```typescript
   async getAccountsWithGranularity(
     granularity: TimeGranularity,
     days: number = 30
   ): Promise<AccountData[]> {
     const endpoint = '/data-analytics/followers/accounts/data/';
     const params = new URLSearchParams({
       granularity: granularity,
       days: days.toString()
     });
     const result = await this.apiClient.get(`${endpoint}?${params}`);
     return result.data;
   }
   ```

3. **数据绑定**
   ```typescript
   <OverviewSection
     accounts={accounts}
     selectedAccIdx={selectedAccIdx}
     granularity={granularity}
     onGranularityChange={setGranularity}
     onAccountChange={setSelectedAccIdx}
   />
   ```

4. **图表渲染**
   ```typescript
   // 粉丝总量积累
   <TrendChart
     data={activeAcc.history[granularity]}
     color="#f8b195"
     type="line"
   />

   // 粉丝净增长
   <TrendChart
     data={activeAcc.history[granularity]}
     color="#8eb69b"
     type="bar"
   />
   ```

---

## 三、时间粒度对比

| 粒度 | 查询范围 | 聚合函数 | 时间格式 | 适用场景 |
|------|---------|---------|---------|---------|
| `HOUR` | 默认 24 小时 | `TruncHour` | `HH:00` | 实时监控、短期波动 |
| `DAY` | 默认 30 天 | `TruncDay` | `MM-DD` | 日常分析、周报 |
| `MONTH` | 默认 365 天 | `TruncMonth` | `YYYY-MM` | 月度总结、年度报告 |

---

## 四、数据完整性保障

### 4.1 定时任务监控

**日志文件**：`logs/bilibili_fans_count.json`

```json
[
  {
    "start_time": "2026-02-01 12:00:00",
    "end_time": "2026-02-01 12:05:00",
    "exit_code": 0,
    "status": "success",
    "error_message": "",
    "summary": "✓ 咻咻满: 1,254,800 粉丝\n✓ 咻小满: 158,000 粉丝\n数据已保存到: data/spider/fans_count/2026/02/b_fans_count_2026-02-01-12.json"
  }
]
```

### 4.2 数据验证

**唯一性约束**：
- 数据库级别：`(account_id, crawl_time)` 唯一索引
- 重复导入自动覆盖，确保数据一致

**数据完整性检查**：
- 粉丝数不能为负数
- 爬取时间不能为未来时间
- 增量计算处理边界情况（第一条记录）

---

## 五、性能优化

### 5.1 数据库索引

```python
# Account 表
models.Index(fields=['platform', 'uid'])
models.Index(fields=['is_active'])

# FollowerMetrics 表
models.Index(fields=['account', 'crawl_time'])
models.Index(fields=['crawl_time'])
```

### 5.2 查询优化

- 使用 `select_related` 减少查询次数
- 使用 `annotate` 聚合数据，避免应用层处理
- 限制查询时间范围，避免全表扫描

### 5.3 缓存策略（可选）

- Redis 缓存热门查询结果
- 设置 5-15 分钟缓存过期时间
- 数据更新时清除缓存

---

## 六、异常处理

### 6.1 爬虫失败

**场景**：B站 API 限流或网络错误

**处理**：
```json
{
  "uid": "37754047",
  "name": "咻咻满",
  "follower": null,
  "status": "error",
  "message": "API rate limit exceeded",
  "timestamp": "2026-02-01 12:00:00"
}
```

**影响**：
- 该账号该时间点数据缺失
- 其他账号不受影响
- 下次爬取尝试补充

### 6.2 API 查询失败

**场景**：数据库连接超时

**前端处理**：
```typescript
try {
  const accounts = await songService.getAccountsWithGranularity(granularity);
} catch (err) {
  setError(err.message);
  // 显示错误提示组件
}
```

### 6.3 数据导入冲突

**场景**：同一时间点重复导入

**处理**：
- 使用 `update_or_create` 自动覆盖
- 记录导入日志，便于追踪

---

## 七、扩展性设计

### 7.1 工具和脚本清单

| 类别 | 工具/脚本 | 位置 | 用途 | 触发方式 |
|------|-----------|------|------|---------|
| **数据采集** | get_bilibili_fans_count.py | spider/ | B站粉丝数爬虫 | 定时任务（每小时） |
| | bilibili_fans_count_cron.sh | scripts/ | 定时任务脚本 | cron（每小时） |
| **数据导入** | ingest_follower_data.py | repo/xxm_fans_backend/tools/ | 数据导入工具 | 自动/手动 |
| **数据库** | db.sqlite3 | data/ | SQLite 数据库 | - |
| **日志** | bilibili_fans_count.json | logs/ | 执行日志 | 自动生成 |
| **API** | follower_service.py | repo/xxm_fans_backend/data_analytics/services/ | 业务逻辑 | 用户访问 |
| | views.py | repo/xxm_fans_backend/data_analytics/api/ | REST API | 用户访问 |

### 7.2 多平台支持

**当前**：仅支持 B站（`platform: 'bilibili'`）

**扩展**：
```python
PLATFORM_CHOICES = [
    ('bilibili', '哔哩哔哩'),
    ('weibo', '微博'),
    ('douyin', '抖音'),
    ('youtube', 'YouTube'),
]
```

### 7.3 多账号管理

**当前**：2 个账号（咻咻满、咻小满）

**扩展**：
- 通过 Django Admin 添加新账号
- 自动开始数据采集
- 无需修改代码

### 7.4 自定义时间范围

**当前**：固定查询天数（days 参数）

**扩展**：
```typescript
// 支持自定义起止时间
async getAccountsWithTimeRange(
  account_id: string,
  start_time: string,
  end_time: string,
  granularity: TimeGranularity
): Promise<AccountData>
```

---

## 八、数据流转图

```
┌─────────────────────────────────────────────────────────────┐
│                      每小时数据流转                           │
└─────────────────────────────────────────────────────────────┘

00:00 ── 爬虫启动 ──> 获取粉丝数 ──> 保存JSON ──> 导入数据库
       │                │              │              │
       │                │              │              ▼
       │                │              │         聚合查询
       │                │              │              │
       │                │              │              ▼
       │                │              │         API返回
       │                │              │              │
       │                │              │              ▼
       │                │              │         前端渲染
       │                │              │
       ▼                ▼              ▼
01:00 ── 爬虫启动 ──> 获取粉丝数 ──> 保存JSON ──> 导入数据库
       │                │              │
       │                │              │
       ▼                ▼              ▼
02:00 ── 爬虫启动 ──> 获取粉丝数 ──> 保存JSON ──> 导入数据库
...（每小时重复）
```

---

## 九、关键时间节点

| 时间点 | 事件 | 涉及组件 |
|--------|------|---------|
| 时间点 | 事件 | 涉及工具 |
|--------|------|---------|
| 每小时:00 | 爬虫启动 | `get_bilibili_fans_count.py` |
| 每小时:00-:02 | 保存JSON | `data/spider/fans_count/` |
| 每小时:02-:03 | 【自动】导入数据 | `ingest_follower_data.py` |
| 每小时:03 | 记录日志 | `logs/bilibili_fans_count.json` |
| 用户访问 | API查询 | REST API + Service |
| 实时 | 前端展示 | React 组件 |

---

## 十、总结

**数据流特点**：
1. **单向流动**：从 B站 API → 爬虫 → 数据库 → API → 前端
2. **全自动流程**：爬虫、导入、日志记录全部自动化，无需人工干预
3. **时间聚合**：按小时/天/月灵活聚合，满足不同分析需求
4. **容错设计**：单个数据点失败不影响整体
5. **易于扩展**：支持多平台、多账号、自定义查询

**数据保障**：
- 定时任务监控
- 数据库唯一性约束
- 前端错误处理
- 自动化日志记录
- 日志记录追踪

---

**文档版本**: 1.0
**创建日期**: 2026-02-01
**最后更新**: 2026-02-01
**作者**: iFlow CLI