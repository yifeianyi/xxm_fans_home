# XXM Fans Home - Agent å¤§æ¨¡å‹æ¥å…¥å®ç°æ–¹æ¡ˆ

## ä¸€ã€æ–¹æ¡ˆæ¦‚è¿°

æœ¬æ–¹æ¡ˆåŸºäºã€ŠAgent æ™ºèƒ½æŸ¥è¯¢å­ç³»ç»Ÿå®ç°æ–¹æ¡ˆã€‹çš„åŸºç¡€ä¸Šï¼Œè¯¦ç»†è§„åˆ’å¦‚ä½•æ¥å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥æå‡ç³»ç»Ÿçš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›å’Œäº¤äº’ä½“éªŒã€‚

### 1.1 ç›®æ ‡

1. **æå‡æ„å›¾è¯†åˆ«å‡†ç¡®ç‡**ï¼šä»è§„åˆ™çš„85%æå‡åˆ°LLMçš„95%+
2. **æ”¯æŒå¤æ‚æŸ¥è¯¢**ï¼šå¤„ç†å¤šæ¡ä»¶ç»„åˆã€æ¨¡ç³Šè¯­ä¹‰ã€ä¸Šä¸‹æ–‡å¼•ç”¨
3. **æ™ºèƒ½æ¨è**ï¼šåŸºäºç”¨æˆ·åå¥½å’Œå†å²æä¾›ä¸ªæ€§åŒ–æ¨è
4. **è‡ªç„¶å¯¹è¯**ï¼šå®ç°æ›´æµç•…çš„å¤šè½®å¯¹è¯ä½“éªŒ
5. **æˆæœ¬å¯æ§**ï¼šé€šè¿‡ç¼“å­˜ã€é™çº§ç­–ç•¥æ§åˆ¶APIè°ƒç”¨æˆæœ¬

### 1.2 æ¥å…¥ç­–ç•¥

é‡‡ç”¨**æ¸è¿›å¼æ¥å…¥**ç­–ç•¥ï¼š
- **é˜¶æ®µä¸€**ï¼šLLMä»…ç”¨äºæ„å›¾è¯†åˆ«å’Œå‚æ•°æå–
- **é˜¶æ®µäºŒ**ï¼šLLMç”¨äºç»“æœè§£é‡Šå’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ
- **é˜¶æ®µä¸‰**ï¼šå®Œå…¨LLMé©±åŠ¨çš„æ™ºèƒ½å¯¹è¯

---

## äºŒã€å¤§æ¨¡å‹é€‰å‹ä¸å¯¹æ¯”

### 2.1 ä¸»æµLLMå¯¹æ¯”

| ç‰¹æ€§ | OpenAI GPT-4 | é€šä¹‰åƒé—® | æ–‡å¿ƒä¸€è¨€ | Claude 3 | DeepSeek-V3 |
|-----|-------------|---------|---------|----------|-------------|
| **ä¸­æ–‡ç†è§£** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **æ¨ç†èƒ½åŠ›** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **APIç¨³å®šæ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **ä»·æ ¼** | $$$ | $$ | $$ | $$$$ | $ |
| **å“åº”é€Ÿåº¦** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 128K | 32K | 8K | 200K | 128K |
| **å›½å†…è®¿é—®** | âŒ éœ€VPN | âœ… | âœ… | âŒ éœ€VPN | âœ… |

### 2.2 æ¨èæ–¹æ¡ˆ

**ä¸»é€‰æ¨¡å‹**ï¼šé€šä¹‰åƒé—® Qwen-Max
- ä¼˜åŠ¿ï¼šä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºã€å›½å†…è®¿é—®ç¨³å®šã€ä»·æ ¼åˆç†
- é€‚ç”¨ï¼šæ„å›¾è¯†åˆ«ã€å‚æ•°æå–ã€ç»“æœè§£é‡Š

**å¤‡é€‰æ¨¡å‹**ï¼šOpenAI GPT-4o mini
- ä¼˜åŠ¿ï¼šæ¨ç†èƒ½åŠ›å¼ºã€APIç”Ÿæ€å®Œå–„
- é€‚ç”¨ï¼šå¤æ‚æ¨ç†ã€å¤šè½®å¯¹è¯

**é™çº§æ–¹æ¡ˆ**ï¼šæœ¬åœ°æ¨¡å‹ï¼ˆQwen-14B-Chat-Int4ï¼‰
- ä¼˜åŠ¿ï¼šé›¶æˆæœ¬ã€éšç§å®‰å…¨
- é€‚ç”¨ï¼šç¦»çº¿åœºæ™¯ã€æˆæœ¬æ•æ„Ÿ

### 2.3 å¤šæ¨¡å‹ç­–ç•¥

```python
# agent/llm/model_selector.py

class ModelSelector:
    """å¤šæ¨¡å‹é€‰æ‹©å™¨"""
    
    # é…ç½®ä¼˜å…ˆçº§
    MODELS = [
        {
            'name': 'qwen-max',
            'provider': 'aliyun',
            'priority': 1,
            'capabilities': ['intent', 'explanation', 'chat'],
            'cost_per_1k_tokens': 0.04
        },
        {
            'name': 'gpt-4o-mini',
            'provider': 'openai',
            'priority': 2,
            'capabilities': ['intent', 'explanation', 'chat', 'reasoning'],
            'cost_per_1k_tokens': 0.15
        },
        {
            'name': 'qwen-14b-local',
            'provider': 'local',
            'priority': 3,
            'capabilities': ['intent'],
            'cost_per_1k_tokens': 0
        }
    ]
    
    @classmethod
    def select_model(cls, capability: str, budget: float = None) -> dict:
        """æ ¹æ®èƒ½åŠ›å’Œé¢„ç®—é€‰æ‹©æ¨¡å‹"""
        available = [m for m in cls.MODELS if capability in m['capabilities']]
        
        if budget:
            available = [m for m in available if m['cost_per_1k_tokens'] <= budget]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        available.sort(key=lambda x: x['priority'])
        
        return available[0] if available else None
```

---

## ä¸‰ã€æ¶æ„è®¾è®¡

### 3.1 LLMé›†æˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·å±‚                                â”‚
â”‚  ç”¨æˆ·æŸ¥è¯¢: "æœ€è¿‘ä¸€ä¸ªæœˆæ¼”å”±çš„å¤é£æ­Œæ›²æœ‰å“ªäº›ï¼Ÿ"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM æ„å›¾è¯†åˆ«å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LLM Client (Qwen-Max)                               â”‚   â”‚
â”‚  â”‚  - æ„å›¾è¯†åˆ«: song_search + filter                    â”‚   â”‚
â”‚  â”‚  - å‚æ•°æå–: {style: "å¤é£", time_range: "last_month"}â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â†“ Redisç¼“å­˜ (intent:hash)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ··åˆæŸ¥è¯¢å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Query Executor                                      â”‚   â”‚
â”‚  â”‚  - è·¯ç”±åˆ°å¯¹åº”çš„æŸ¥è¯¢æœåŠ¡                               â”‚   â”‚
â”‚  â”‚  - æ‰§è¡ŒDjango ORMæŸ¥è¯¢                                 â”‚   â”‚
â”‚  â”‚  - è¿”å›ç»“æ„åŒ–æ•°æ®                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM ç»“æœç”Ÿæˆå±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LLM Client (Qwen-Turbo - æ›´å¿«æ›´ä¾¿å®œ)                 â”‚   â”‚
â”‚  â”‚  - å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€                         â”‚   â”‚
â”‚  â”‚  - ç”Ÿæˆå‹å¥½çš„å›å¤æ–‡æœ¬                                 â”‚   â”‚
â”‚  â”‚  - æ·»åŠ è§£é‡Šå’Œå»ºè®®                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â†“ Redisç¼“å­˜ (response:hash)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å“åº”å±‚                                â”‚
â”‚  {                                                             â”‚
â”‚    "success": true,                                           â”‚
â”‚    "response": "æœ€è¿‘ä¸€ä¸ªæœˆï¼Œå’»å’»æ»¡æ¼”å”±äº†5é¦–å¤é£æ­Œæ›²ï¼š...",    â”‚
â”‚    "data": [...],                                            â”‚
â”‚    "model_used": "qwen-max"                                 â”‚
â”‚  }                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æµå¯¹æ¯”

| é˜¶æ®µ | è§„åˆ™å¼•æ“æ–¹æ¡ˆ | LLMæ–¹æ¡ˆ |
|-----|------------|---------|
| **æ„å›¾è¯†åˆ«** | æ­£åˆ™åŒ¹é… (0.05s) | LLMè°ƒç”¨ (0.5s) |
| **å‚æ•°æå–** | æ­£åˆ™æå– (0.01s) | LLMæå– (åŒ…å«åœ¨æ„å›¾è¯†åˆ«ä¸­) |
| **æŸ¥è¯¢æ‰§è¡Œ** | Django ORM (0.12s) | Django ORM (0.12s) |
| **ç»“æœç”Ÿæˆ** | æ¨¡æ¿æ¸²æŸ“ (0.01s) | LLMç”Ÿæˆ (0.3s) |
| **æ€»è€—æ—¶** | 0.19s | 0.92s |
| **å‡†ç¡®ç‡** | 85% | 95%+ |

---

## å››ã€Promptå·¥ç¨‹è®¾è®¡

### 4.1 æ„å›¾è¯†åˆ«Prompt

```python
# agent/llm/prompts/intent_recognition.py

INTENT_RECOGNITION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å’»å’»æ»¡ï¼ˆXXMï¼‰çš„éŸ³ä¹ã€ç›´æ’­ã€äºŒåˆ›ä½œå“ç­‰æ•°æ®ã€‚

## ä»»åŠ¡
åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œè¯†åˆ«æŸ¥è¯¢æ„å›¾å¹¶æå–å‚æ•°ã€‚

## æ”¯æŒçš„æ„å›¾ç±»å‹

1. **song_search** - æ­Œæ›²æœç´¢
   - å‚æ•°ï¼šsong_name, keywords, style, tags, language, singer
   - ç¤ºä¾‹ï¼š"æœç´¢ã€Šç¨»é¦™ã€‹" â†’ song_name="ç¨»é¦™"

2. **song_stats** - æ­Œæ›²ç»Ÿè®¡
   - å‚æ•°ï¼šyear, month, time_range, limit, sort_by
   - ç¤ºä¾‹ï¼š"2024å¹´å”±å¾—æœ€å¤šçš„5é¦–æ­Œ" â†’ year=2024, limit=5

3. **record_search** - æ¼”å”±è®°å½•æŸ¥è¯¢
   - å‚æ•°ï¼šsong_name, date, date_range, limit
   - ç¤ºä¾‹ï¼š"æ‰¾ä¸€ä¸‹2023å¹´12æœˆ25æ—¥çš„æ¼”å”±è®°å½•" â†’ date="2023-12-25"

4. **livestream_search** - ç›´æ’­æŸ¥è¯¢
   - å‚æ•°ï¼šdate, date_range, time_range, limit
   - ç¤ºä¾‹ï¼š"ä¸Šå‘¨æœ‰å“ªäº›ç›´æ’­ï¼Ÿ" â†’ time_range="last_week"

5. **fansdiy_search** - äºŒåˆ›ä½œå“æœç´¢
   - å‚æ•°ï¼škeywords, author, collection, limit
   - ç¤ºä¾‹ï¼š"æ‰¾æ‰¾å’»å’»æ»¡ç›¸å…³çš„äºŒåˆ›è§†é¢‘" â†’ keywords="å’»å’»æ»¡"

6. **gallery_search** - å›¾é›†æŸ¥è¯¢
   - å‚æ•°ï¼škeywords, date, date_range, category, limit
   - ç¤ºä¾‹ï¼š"2024å¹´çš„æ¼”å”±ä¼šå›¾ç‰‡" â†’ year=2024

7. **analytics_query** - æ•°æ®åˆ†æ
   - å‚æ•°ï¼šmetric, time_range, account_name
   - ç¤ºä¾‹ï¼š"ç²‰ä¸æ•°å¢é•¿æœ€å¿«çš„æœˆä»½" â†’ metric="fans_growth"

8. **comparison** - å¯¹æ¯”æŸ¥è¯¢
   - å‚æ•°ï¼šmetric, value1, value2, time_range
   - ç¤ºä¾‹ï¼š"å¯¹æ¯”2023å’Œ2024å¹´çš„æ¼”å”±æ¬¡æ•°" â†’ metric="perform_count", value1=2023, value2=2024

9. **recommendation** - æ¨èæŸ¥è¯¢
   - å‚æ•°ï¼šstyle, tags, limit
   - ç¤ºä¾‹ï¼š"æ¨èä¸€äº›å¤é£æ­Œæ›²" â†’ style="å¤é£"

10. **unknown** - æœªçŸ¥æ„å›¾
    - å½“æ— æ³•è¯†åˆ«æ—¶ä½¿ç”¨

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
```json
{
  "intent_type": "æ„å›¾ç±»å‹",
  "parameters": {
    "å‚æ•°å": "å‚æ•°å€¼"
  },
  "confidence": 0.95,
  "reasoning": "ç®€è¦è¯´æ˜è¯†åˆ«ç†ç”±"
}
```

## ç”¨æˆ·æŸ¥è¯¢
{query}

## ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
{context}

## è¾“å‡º
"""
```

### 4.2 ç»“æœç”ŸæˆPrompt

```python
# agent/llm/prompts/result_generation.py

RESULT_GENERATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„éŸ³ä¹åŠ©æ‰‹ï¼Œæ­£åœ¨ä¸ºç”¨æˆ·å±•ç¤ºæŸ¥è¯¢ç»“æœã€‚

## ä»»åŠ¡
å°†ç»“æ„åŒ–çš„æŸ¥è¯¢æ•°æ®è½¬æ¢ä¸ºè‡ªç„¶ã€å‹å¥½çš„ä¸­æ–‡å›å¤ã€‚

## å›å¤è¦æ±‚
1. è¯­æ°”äº²åˆ‡è‡ªç„¶ï¼Œåƒç²‰ä¸ä¹‹é—´çš„äº¤æµ
2. çªå‡ºå…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨é€‚å½“çš„emojiè¡¨æƒ…
3. å¦‚æœæ•°æ®è¾ƒå¤šï¼Œè¿›è¡Œé€‚å½“æ€»ç»“
4. æä¾›ç›¸å…³çš„å»¶ä¼¸å»ºè®®
5. ä¿æŒå›å¤ç®€æ´ï¼Œä¸è¶…è¿‡200å­—

## æŸ¥è¯¢ç»“æœæ•°æ®
{data}

## æŸ¥è¯¢æ„å›¾
{intent}

## ç”¨æˆ·æŸ¥è¯¢åŸæ–‡
{query}

## è¾“å‡ºæ ¼å¼
```json
{
  "response": "è‡ªç„¶è¯­è¨€å›å¤",
  "highlights": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2"],
  "suggestions": ["å»ºè®®1", "å»ºè®®2"]
}
```

## è¾“å‡º
"""
```

### 4.3 ä¸Šä¸‹æ–‡å¢å¼ºPrompt

```python
# agent/llm/prompts/context_enhancement.py

CONTEXT_ENHANCEMENT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡ç†è§£åŠ©æ‰‹ï¼Œå¸®åŠ©è§£æå¤šè½®å¯¹è¯ä¸­çš„æŒ‡ä»£å…³ç³»ã€‚

## ä»»åŠ¡
æ ¹æ®å¯¹è¯å†å²ï¼Œè§£æå½“å‰æŸ¥è¯¢ä¸­çš„æŒ‡ä»£è¯ï¼ˆå¦‚"è¿™é¦–æ­Œ"ã€"åˆšæ‰"ã€"å®ƒ"ç­‰ï¼‰ã€‚

## å¯¹è¯å†å²
{history}

## å½“å‰æŸ¥è¯¢
{current_query}

## å¯ç”¨çš„å®ä½“
{entities}

## è¾“å‡ºæ ¼å¼
```json
{
  "resolved_query": "è§£æåçš„å®Œæ•´æŸ¥è¯¢",
  "resolved_parameters": {
    "å‚æ•°å": "è§£æåçš„å€¼"
  },
  "references": ["å¼•ç”¨çš„å†å²å®ä½“"]
}
```

## è¾“å‡º
"""
```

### 4.4 Promptä¼˜åŒ–ç­–ç•¥

**1. Few-Shot Learningï¼ˆå°‘æ ·æœ¬å­¦ä¹ ï¼‰**

```python
INTENT_RECOGNITION_PROMPT_WITH_EXAMPLES = """
...ï¼ˆå‰æ–‡åŒä¸Šï¼‰...

## ç¤ºä¾‹

ç¤ºä¾‹1:
æŸ¥è¯¢: "2024å¹´å”±å¾—æœ€å¤šçš„5é¦–æ­Œ"
è¾“å‡º:
{
  "intent_type": "song_stats",
  "parameters": {"year": 2024, "limit": 5, "sort_by": "perform_count"},
  "confidence": 0.98,
  "reasoning": "æ˜ç¡®æåˆ°äº†å¹´ä»½2024å’Œæ•°é‡5ï¼Œè¦æ±‚ç»Ÿè®¡æ¼”å”±æ¬¡æ•°æœ€å¤šçš„æ­Œæ›²"
}

ç¤ºä¾‹2:
æŸ¥è¯¢: "ä¸Šå‘¨æœ‰å“ªäº›ç›´æ’­ï¼Ÿ"
è¾“å‡º:
{
  "intent_type": "livestream_search",
  "parameters": {"time_range": "last_week"},
  "confidence": 0.95,
  "reasoning": "è¯¢é—®'ä¸Šå‘¨'çš„ç›´æ’­ï¼Œä½¿ç”¨time_rangeå‚æ•°"
}

ç¤ºä¾‹3:
æŸ¥è¯¢: "è¿™é¦–æ­Œç¬¬ä¸€æ¬¡å”±æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ"
è¾“å‡º:
{
  "intent_type": "record_search",
  "parameters": {"song_name": "{current_entity}", "sort_by": "first_perform"},
  "confidence": 0.90,
  "reasoning": "å¼•ç”¨äº†'è¿™é¦–æ­Œ'ï¼Œéœ€è¦ä»ä¸Šä¸‹æ–‡è·å–å®ä½“"
}

## å½“å‰æŸ¥è¯¢
{query}

## è¾“å‡º
"""
```

**2. Chain-of-Thoughtï¼ˆæ€ç»´é“¾ï¼‰**

```python
INTENT_RECOGNITION_PROMPT_WITH_COT = """
...ï¼ˆå‰æ–‡åŒä¸Šï¼‰...

## æ€è€ƒæ­¥éª¤
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š
1. è¯†åˆ«æŸ¥è¯¢ä¸­çš„å…³é”®è¯
2. åˆ¤æ–­å…³é”®è¯åŒ¹é…çš„æ„å›¾ç±»å‹
3. æå–æŸ¥è¯¢ä¸­çš„å…·ä½“å‚æ•°å€¼
4. è¯„ä¼°è¯†åˆ«çš„ç½®ä¿¡åº¦
5. ç»™å‡ºè¯†åˆ«ç†ç”±

## ç”¨æˆ·æŸ¥è¯¢
{query}

## é€æ­¥åˆ†æ

## è¾“å‡º
"""
```

---

## äº”ã€æ ¸å¿ƒå®ç°

### 5.1 LLMå®¢æˆ·ç«¯å°è£…

```python
# agent/llm/llm_client.py

import os
import json
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod
import dashscope
from openai import OpenAI

class LLMProvider(ABC):
    """LLMæä¾›å•†æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def chat(self, messages: list, **kwargs) -> str:
        """èŠå¤©æ¥å£"""
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        pass

class QwenProvider(LLMProvider):
    """é€šä¹‰åƒé—®æä¾›å•†"""
    
    def __init__(self, api_key: str = None):
        dashscope.api_key = api_key or os.getenv('DASHSCOPE_API_KEY')
        self.model = "qwen-max"
    
    def chat(self, messages: list, **kwargs) -> str:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        response = dashscope.Generation.call(
            model=self.model,
            messages=messages,
            result_format='message',
            **kwargs
        )
        
        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")
    
    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦1.5å­—ç¬¦=1tokenï¼‰"""
        return len(text) // 1.5

class OpenAIProvider(LLMProvider):
    """OpenAIæä¾›å•†"""
    
    def __init__(self, api_key: str = None):
        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        self.model = "gpt-4o-mini"
    
    def chat(self, messages: list, **kwargs) -> str:
        """è°ƒç”¨OpenAI API"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        
        return response.choices[0].message.content
    
    def count_tokens(self, text: str) -> int:
        """ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°é‡"""
        import tiktoken
        encoding = tiktoken.encoding_for_model(self.model)
        return len(encoding.encode(text))

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼ˆç»Ÿä¸€æ¥å£ï¼‰"""
    
    def __init__(self, provider: LLMProvider = None):
        self.provider = provider or self._get_default_provider()
        self.cache_enabled = True
    
    def _get_default_provider(self) -> LLMProvider:
        """è·å–é»˜è®¤æä¾›å•†"""
        # ä¼˜å…ˆä½¿ç”¨é€šä¹‰åƒé—®
        if os.getenv('DASHSCOPE_API_KEY'):
            return QwenProvider()
        # å…¶æ¬¡ä½¿ç”¨OpenAI
        elif os.getenv('OPENAI_API_KEY'):
            return OpenAIProvider()
        # æœ€åé™çº§åˆ°æœ¬åœ°æ¨¡å‹
        else:
            return LocalLLMProvider()
    
    def complete(self, prompt: str, system_prompt: str = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """å®Œæˆæ–‡æœ¬ç”Ÿæˆ"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled:
            cache_key = self._generate_cache_key(prompt, system_prompt)
            cached = self._get_from_cache(cache_key)
            if cached:
                return cached
        
        # è°ƒç”¨API
        response = self.provider.chat(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # ç¼“å­˜ç»“æœ
        if self.cache_enabled:
            self._save_to_cache(cache_key, response)
        
        return response
    
    def json_complete(self, prompt: str, system_prompt: str = None,
                      temperature: float = 0.3, max_tokens: int = 1000) -> dict:
        """å®ŒæˆJSONç”Ÿæˆ"""
        # å¼ºåˆ¶è¦æ±‚è¿”å›JSON
        if not system_prompt:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚"
        
        response = self.complete(prompt, system_prompt, temperature, max_tokens)
        
        try:
            # å°è¯•è§£æJSON
            # æå–JSONéƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            json_str = self._extract_json(response)
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤
            return self._fix_and_parse_json(response)
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        text = text.strip()
        if text.startswith('```json'):
            text = text[7:]
        if text.startswith('```'):
            text = text[3:]
        if text.endswith('```'):
            text = text[:-3]
        return text.strip()
    
    def _fix_and_parse_json(self, text: str) -> dict:
        """ä¿®å¤å¹¶è§£æJSON"""
        # ç®€å•çš„ä¿®å¤ç­–ç•¥
        text = self._extract_json(text)
        
        # å°è¯•ä½¿ç”¨æ­£åˆ™æå–JSONå¯¹è±¡
        import re
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        
        # æœ€åé™çº§ï¼šè¿”å›é»˜è®¤ç»“æ„
        return {
            "error": "JSONè§£æå¤±è´¥",
            "raw_response": text
        }
    
    def _generate_cache_key(self, prompt: str, system_prompt: str = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content = f"{system_prompt or ''}:{prompt}"
        return f"llm:{hashlib.md5(content.encode()).hexdigest()}"
    
    def _get_from_cache(self, key: str) -> Optional[str]:
        """ä»ç¼“å­˜è·å–"""
        from django.core.cache import cache
        return cache.get(key)
    
    def _save_to_cache(self, key: str, value: str, timeout: int = 3600):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        from django.core.cache import cache
        cache.set(key, value, timeout)
```

### 5.2 LLMæ„å›¾è¯†åˆ«å™¨

```python
# agent/intent_recognizer/llm_recognizer.py

from enum import Enum
from typing import Dict, Any
from agent.llm.llm_client import LLMClient
from agent.llm.prompts.intent_recognition import INTENT_RECOGNITION_PROMPT

class IntentType(Enum):
    """æ„å›¾ç±»å‹æšä¸¾"""
    SONG_SEARCH = "song_search"
    SONG_STATS = "song_stats"
    RECORD_SEARCH = "record_search"
    LIVESTREAM_SEARCH = "livestream_search"
    FANSDIY_SEARCH = "fansdiy_search"
    GALLERY_SEARCH = "gallery_search"
    ANALYTICS_QUERY = "analytics_query"
    COMPARISON = "comparison"
    RECOMMENDATION = "recommendation"
    UNKNOWN = "unknown"

class Intent:
    """æ„å›¾å¯¹è±¡"""
    
    def __init__(self, type: IntentType, parameters: Dict[str, Any], 
                 confidence: float, reasoning: str = ""):
        self.type = type
        self.parameters = parameters
        self.confidence = confidence
        self.reasoning = reasoning
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "type": self.type.value,
            "parameters": self.parameters,
            "confidence": self.confidence,
            "reasoning": self.reasoning
        }

class LLMIntentRecognizer:
    """åŸºäºLLMçš„æ„å›¾è¯†åˆ«å™¨"""
    
    def __init__(self, llm_client: LLMClient = None):
        self.llm_client = llm_client or LLMClient()
        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éŸ³ä¹æ•°æ®æŸ¥è¯¢æ„å›¾è¯†åˆ«ç³»ç»Ÿã€‚
è¯·å‡†ç¡®è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼Œå¹¶æå–ç›¸å…³å‚æ•°ã€‚
"""
    
    def recognize(self, query: str, context: Dict[str, Any] = None) -> Intent:
        """è¯†åˆ«æŸ¥è¯¢æ„å›¾"""
        # æ„å»ºæç¤ºè¯
        context_str = json.dumps(context, ensure_ascii=False) if context else "æ— "
        
        prompt = INTENT_RECOGNITION_PROMPT.format(
            query=query,
            context=context_str
        )
        
        try:
            # è°ƒç”¨LLM
            result = self.llm_client.json_complete(
                prompt=prompt,
                system_prompt=self.system_prompt,
                temperature=0.3,  # è¾ƒä½æ¸©åº¦ï¼Œæé«˜ç¡®å®šæ€§
                max_tokens=500
            )
            
            # è§£æç»“æœ
            intent_type_str = result.get('intent_type', 'unknown')
            intent_type = IntentType(intent_type_str)
            
            parameters = result.get('parameters', {})
            confidence = result.get('confidence', 0.0)
            reasoning = result.get('reasoning', '')
            
            # å‚æ•°ç±»å‹è½¬æ¢
            parameters = self._convert_parameters(parameters)
            
            return Intent(
                type=intent_type,
                parameters=parameters,
                confidence=confidence,
                reasoning=reasoning
            )
            
        except Exception as e:
            # é™çº§åˆ°è§„åˆ™å¼•æ“
            from agent.intent_recognizer.rule_based import RuleBasedIntentRecognizer
            fallback = RuleBasedIntentRecognizer()
            return fallback.recognize(query)
    
    def _convert_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢å‚æ•°ç±»å‹"""
        converted = {}
        
        for key, value in parameters.items():
            # è½¬æ¢å¹´ä»½
            if key in ['year', 'value1', 'value2'] and isinstance(value, str):
                try:
                    converted[key] = int(value)
                except ValueError:
                    converted[key] = value
            # è½¬æ¢æ•°é‡é™åˆ¶
            elif key in ['limit'] and isinstance(value, str):
                try:
                    converted[key] = int(value)
                except ValueError:
                    converted[key] = 10  # é»˜è®¤å€¼
            # è½¬æ¢æ ‡ç­¾åˆ—è¡¨
            elif key in ['tags'] and isinstance(value, str):
                converted[key] = [tag.strip() for tag in value.split(',')]
            else:
                converted[key] = value
        
        return converted
    
    def batch_recognize(self, queries: list) -> list[Intent]:
        """æ‰¹é‡è¯†åˆ«æ„å›¾"""
        return [self.recognize(query) for query in queries]
```

### 5.3 ä¸Šä¸‹æ–‡è§£æå™¨

```python
# agent/context_manager/context_parser.py

from typing import Dict, Any, List
from agent.llm.llm_client import LLMClient
from agent.llm.prompts.context_enhancement import CONTEXT_ENHANCEMENT_PROMPT

class ContextParser:
    """ä¸Šä¸‹æ–‡è§£æå™¨"""
    
    def __init__(self, llm_client: LLMClient = None):
        self.llm_client = llm_client or LLMClient()
    
    def resolve_references(self, current_query: str, 
                           history: List[Dict[str, Any]],
                           entities: List[str] = None) -> Dict[str, Any]:
        """è§£ææŸ¥è¯¢ä¸­çš„å¼•ç”¨"""
        entities = entities or []
        
        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_str = self._format_history(history)
        
        # æ„å»ºå®ä½“å­—ç¬¦ä¸²
        entities_str = ", ".join(entities) if entities else "æ— "
        
        prompt = CONTEXT_ENHANCEMENT_PROMPT.format(
            current_query=current_query,
            history=history_str,
            entities=entities_str
        )
        
        try:
            result = self.llm_client.json_complete(
                prompt=prompt,
                temperature=0.3,
                max_tokens=300
            )
            
            return {
                "resolved_query": result.get("resolved_query", current_query),
                "resolved_parameters": result.get("resolved_parameters", {}),
                "references": result.get("references", [])
            }
        except Exception:
            # é™çº§ï¼šç®€å•çš„æ›¿æ¢è§„åˆ™
            return self._simple_resolve(current_query, entities)
    
    def _format_history(self, history: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        if not history:
            return "æ— å†å²å¯¹è¯"
        
        lines = []
        for msg in history[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡
            role = "ç”¨æˆ·" if msg['role'] == 'user' else "åŠ©æ‰‹"
            lines.append(f"{role}: {msg['content']}")
        
        return "\n".join(lines)
    
    def _simple_resolve(self, query: str, entities: List[str]) -> Dict[str, Any]:
        """ç®€å•çš„å¼•ç”¨è§£æ"""
        resolved = query
        references = []
        
        # å¸¸è§æŒ‡ä»£è¯æ›¿æ¢
        replacements = {
            "è¿™é¦–æ­Œ": entities[0] if entities else "",
            "é‚£é¦–æ­Œ": entities[1] if len(entities) > 1 else "",
            "å®ƒ": entities[0] if entities else "",
        }
        
        for ref, entity in replacements.items():
            if entity and ref in query:
                resolved = resolved.replace(ref, entity)
                references.append(entity)
        
        return {
            "resolved_query": resolved,
            "resolved_parameters": {},
            "references": references
        }
```

### 5.4 ç»“æœç”Ÿæˆå™¨

```python
# agent/result_generator/response_generator.py

from typing import Dict, Any
from agent.llm.llm_client import LLMClient
from agent.llm.prompts.result_generation import RESULT_GENERATION_PROMPT

class ResponseGenerator:
    """å“åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client: LLMClient = None):
        self.llm_client = llm_client or LLMClient()
        # ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
        if hasattr(self.llm_client.provider, 'model'):
            self.llm_client.provider.model = "qwen-turbo"
    
    def generate(self, query: str, intent: Dict[str, Any], 
                 data: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆè‡ªç„¶è¯­è¨€å“åº”"""
        prompt = RESULT_GENERATION_PROMPT.format(
            data=json.dumps(data, ensure_ascii=False),
            intent=json.dumps(intent, ensure_ascii=False),
            query=query
        )
        
        try:
            result = self.llm_client.json_complete(
                prompt=prompt,
                temperature=0.8,  # è¾ƒé«˜æ¸©åº¦ï¼Œå¢åŠ å¤šæ ·æ€§
                max_tokens=300
            )
            
            return {
                "response": result.get("response", ""),
                "highlights": result.get("highlights", []),
                "suggestions": result.get("suggestions", [])
            }
        except Exception:
            # é™çº§åˆ°æ¨¡æ¿
            return self._template_generate(data)
    
    def _template_generate(self, data: Dict[str, Any]) -> Dict[str, str]:
        """æ¨¡æ¿ç”Ÿæˆï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        if data.get("type") == "table":
            count = len(data.get("rows", []))
            summary = data.get("summary", "")
            return {
                "response": f"ä¸ºæ‚¨æ‰¾åˆ°äº†{count}æ¡{summary}ï¼Œè¯¦ç»†æ•°æ®è§ä¸‹è¡¨ï¼š",
                "highlights": [f"å…±{count}æ¡è®°å½•"],
                "suggestions": ["æ‚¨å¯ä»¥è¿›ä¸€æ­¥ç­›é€‰æ•°æ®", "æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"]
            }
        
        return {
            "response": "æŸ¥è¯¢å®Œæˆï¼Œè¯·æŸ¥çœ‹ç»“æœ",
            "highlights": [],
            "suggestions": []
        }
```

---

## å…­ã€æˆæœ¬æ§åˆ¶ç­–ç•¥

### 6.1 æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ

```python
# agent/cost/cost_optimizer.py

class CostOptimizer:
    """æˆæœ¬ä¼˜åŒ–å™¨"""
    
    # Tokenæˆæœ¬ï¼ˆæ¯1k tokensï¼Œå•ä½ï¼šå…ƒï¼‰
    COSTS = {
        "qwen-max": {"input": 0.04, "output": 0.12},
        "qwen-turbo": {"input": 0.008, "output": 0.02},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60}
    }
    
    def __init__(self, daily_budget: float = 50.0):
        self.daily_budget = daily_budget
        self.daily_spent = 0.0
    
    def should_use_llm(self, confidence: float = None, 
                       complexity: str = "medium") -> bool:
        """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨LLM"""
        # ç®€å•æŸ¥è¯¢ä½¿ç”¨è§„åˆ™å¼•æ“
        if complexity == "simple":
            return False
        
        # æ£€æŸ¥é¢„ç®—
        if self.daily_spent >= self.daily_budget:
            return False
        
        # é«˜ç½®ä¿¡åº¦è§„åˆ™åŒ¹é…ï¼Œå¯ä»¥è·³è¿‡LLM
        if confidence and confidence > 0.95:
            return False
        
        return True
    
    def select_model(self, task: str, budget: float = None) -> str:
        """é€‰æ‹©æ¨¡å‹"""
        budget = budget or (self.daily_budget - self.daily_spent)
        
        # æ„å›¾è¯†åˆ«ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
        if task == "intent_recognition":
            return "qwen-turbo"
        
        # ç»“æœç”Ÿæˆä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
        if task == "response_generation":
            return "qwen-turbo"
        
        # å¤æ‚æ¨ç†ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
        if task == "complex_reasoning":
            if budget > 5.0:
                return "qwen-max"
            else:
                return "qwen-turbo"
        
        return "qwen-turbo"
    
    def calculate_cost(self, model: str, input_tokens: int, 
                       output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        costs = self.COSTS.get(model, {"input": 0, "output": 0})
        input_cost = (input_tokens / 1000) * costs["input"]
        output_cost = (output_tokens / 1000) * costs["output"]
        return input_cost + output_cost
    
    def record_cost(self, cost: float):
        """è®°å½•æˆæœ¬"""
        self.daily_spent += cost
```

### 6.2 ç¼“å­˜ç­–ç•¥

```python
# agent/cache/llm_cache.py

from django.core.cache import cache
from typing import Optional
import hashlib
import json

class LLMCache:
    """LLMä¸“ç”¨ç¼“å­˜"""
    
    # ä¸åŒä»»åŠ¡çš„ç¼“å­˜æ—¶é—´
    CACHE_TIMEOUTS = {
        "intent_recognition": 86400,  # 1å¤© - æ„å›¾è¯†åˆ«ç»“æœç¨³å®š
        "response_generation": 3600,  # 1å°æ—¶ - å“åº”å¯ä»¥ä¸ªæ€§åŒ–
        "context_enhancement": 1800,  # 30åˆ†é’Ÿ - ä¸Šä¸‹æ–‡å˜åŒ–è¾ƒå¿«
    }
    
    @classmethod
    def get(cls, task: str, prompt: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        key = cls._generate_key(task, prompt)
        return cache.get(key)
    
    @classmethod
    def set(cls, task: str, prompt: str, result: str):
        """è®¾ç½®ç¼“å­˜"""
        key = cls._generate_key(task, prompt)
        timeout = cls.CACHE_TIMEOUTS.get(task, 3600)
        cache.set(key, result, timeout)
    
    @classmethod
    def invalidate_pattern(cls, pattern: str):
        """æ‰¹é‡å¤±æ•ˆ"""
        keys = cache.keys(f"llm:{pattern}:*")
        for key in keys:
            cache.delete(key)
    
    @classmethod
    def _generate_key(cls, task: str, prompt: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        hash_value = hashlib.md5(prompt.encode()).hexdigest()
        return f"llm:{task}:{hash_value}"
```

### 6.3 æˆæœ¬ç›‘æ§

```python
# agent/monitoring/cost_monitor.py

from django.db import models
from django.utils import timezone

class LLMApiUsage(models.Model):
    """LLM APIä½¿ç”¨è®°å½•"""
    
    model = models.CharField(max_length=50, verbose_name="æ¨¡å‹")
    task = models.CharField(max_length=50, verbose_name="ä»»åŠ¡ç±»å‹")
    input_tokens = models.IntegerField(verbose_name="è¾“å…¥tokens")
    output_tokens = models.IntegerField(verbose_name="è¾“å‡ºtokens")
    cost = models.FloatField(verbose_name="æˆæœ¬(å…ƒ)")
    cache_hit = models.BooleanField(default=False, verbose_name="æ˜¯å¦å‘½ä¸­ç¼“å­˜")
    created_at = models.DateTimeField(auto_now_add=True, verbose_name="æ—¶é—´")
    
    class Meta:
        db_table = 'llm_api_usage'
        verbose_name = "LLM APIä½¿ç”¨è®°å½•"
        ordering = ['-created_at']

class CostMonitor:
    """æˆæœ¬ç›‘æ§å™¨"""
    
    @classmethod
    def record_usage(cls, model: str, task: str, input_tokens: int, 
                     output_tokens: int, cost: float, cache_hit: bool):
        """è®°å½•APIä½¿ç”¨"""
        LLMApiUsage.objects.create(
            model=model,
            task=task,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost=cost,
            cache_hit=cache_hit
        )
    
    @classmethod
    def get_daily_cost(cls, date: timezone.datetime = None) -> dict:
        """è·å–æ¯æ—¥æˆæœ¬"""
        date = date or timezone.now().date()
        
        usage = LLMApiUsage.objects.filter(
            created_at__date=date
        ).aggregate(
            total_cost=models.Sum('cost'),
            total_tokens=models.Sum(models.F('input_tokens') + models.F('output_tokens')),
            cache_hits=models.Count('id', filter=models.Q(cache_hit=True)),
            total_requests=models.Count('id')
        )
        
        return {
            "date": date,
            "total_cost": usage['total_cost'] or 0,
            "total_tokens": usage['total_tokens'] or 0,
            "cache_hits": usage['cache_hits'] or 0,
            "total_requests": usage['total_requests'] or 0,
            "cache_hit_rate": (usage['cache_hits'] / usage['total_requests'] * 100) 
                              if usage['total_requests'] else 0
        }
```

---

## ä¸ƒã€æ··åˆæŸ¥è¯¢ç­–ç•¥

### 7.1 è§„åˆ™+LLMæ··åˆå¼•æ“

```python
# agent/intent_recognizer/hybrid_recognizer.py

from agent.intent_recognizer.llm_recognizer import LLMIntentRecognizer
from agent.intent_recognizer.rule_based import RuleBasedIntentRecognizer

class HybridIntentRecognizer:
    """æ··åˆæ„å›¾è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.rule_recognizer = RuleBasedIntentRecognizer()
        self.llm_recognizer = LLMIntentRecognizer()
        self.cost_optimizer = CostOptimizer(daily_budget=50.0)
    
    def recognize(self, query: str, context: dict = None) -> Intent:
        """æ··åˆè¯†åˆ«"""
        # 1. å…ˆç”¨è§„åˆ™å¼•æ“å¿«é€Ÿè¯†åˆ«
        rule_result = self.rule_recognizer.recognize(query)
        
        # 2. è¯„ä¼°è§„åˆ™è¯†åˆ«ç»“æœ
        if rule_result.confidence > 0.95:
            # é«˜ç½®ä¿¡åº¦ï¼Œç›´æ¥è¿”å›
            return rule_result
        
        if rule_result.confidence < 0.6:
            # ä½ç½®ä¿¡åº¦ï¼Œå¿…é¡»ä½¿ç”¨LLM
            if self.cost_optimizer.should_use_llm(complexity="medium"):
                return self.llm_recognizer.recognize(query, context)
            else:
                # é¢„ç®—ä¸è¶³ï¼Œè¿”å›è§„åˆ™ç»“æœ
                return rule_result
        
        # 3. ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæ ¹æ®å¤æ‚åº¦å†³å®š
        complexity = self._assess_complexity(query)
        
        if complexity == "simple":
            return rule_result
        elif complexity == "medium" and self.cost_optimizer.should_use_llm():
            return self.llm_recognizer.recognize(query, context)
        else:
            return rule_result
    
    def _assess_complexity(self, query: str) -> str:
        """è¯„ä¼°æŸ¥è¯¢å¤æ‚åº¦"""
        # ç®€å•æŸ¥è¯¢ï¼šå•ä¸€æ„å›¾ï¼Œæ˜ç¡®å‚æ•°
        simple_patterns = [
            r'^\d{4}å¹´å”±å¾—æœ€å¤šçš„\d+é¦–æ­Œ$',
            r'^æœç´¢ã€Š.+ã€‹$',
            r'^ä¸Šå‘¨æœ‰å“ªäº›ç›´æ’­\?$'
        ]
        
        for pattern in simple_patterns:
            if re.match(pattern, query):
                return "simple"
        
        # å¤æ‚æŸ¥è¯¢ï¼šå¤šæ¡ä»¶ç»„åˆ
        if 'å’Œ' in query or 'æˆ–è€…' in query or 'å¯¹æ¯”' in query:
            return "complex"
        
        return "medium"
```

### 7.2 æŸ¥è¯¢æ‰§è¡Œä¼˜åŒ–

```python
# agent/query_executor/optimized_executor.py

class OptimizedQueryExecutor:
    """ä¼˜åŒ–çš„æŸ¥è¯¢æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.services = {...}
        self.cache = CacheManager()
        self.llm_cache = LLMCache()
    
    def execute(self, intent: Intent) -> QueryResult:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # 1. æ£€æŸ¥ç»“æœç¼“å­˜
        cache_key = self._generate_cache_key(intent)
        cached_result = self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # 2. æ‰§è¡ŒæŸ¥è¯¢
        result = self._execute_query(intent)
        
        # 3. å¦‚æœæˆåŠŸï¼Œç¼“å­˜ç»“æœ
        if result.success:
            self.cache.set(cache_key, result, timeout=300)
        
        return result
    
    def _execute_query(self, intent: Intent) -> QueryResult:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        try:
            # æ ¹æ®æ„å›¾ç±»å‹è·¯ç”±
            if intent.type == IntentType.SONG_SEARCH:
                return self._execute_song_search(intent.parameters)
            elif intent.type == IntentType.SONG_STATS:
                return self._execute_song_stats(intent.parameters)
            # ... å…¶ä»–æ„å›¾
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return QueryResult(success=False, error=str(e))
    
    def _execute_song_search(self, params: dict) -> QueryResult:
        """æ‰§è¡Œæ­Œæ›²æœç´¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # 1. æ„å»ºæŸ¥è¯¢
        query = Song.objects.all()
        
        # 2. åº”ç”¨è¿‡æ»¤å™¨ï¼ˆä½¿ç”¨select_relatedå‡å°‘æŸ¥è¯¢ï¼‰
        if params.get('style'):
            query = query.filter(
                song_styles__style__name=params['style']
            ).select_related()
        
        if params.get('tags'):
            query = query.filter(
                song_tags__tag__name__in=params['tags']
            ).prefetch_related('song_tags__tag')
        
        # 3. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨annotateè®¡ç®—ç»Ÿè®¡ï¼‰
        songs = query.annotate(
            perform_count=Count('records')
        ).order_by('-perform_count')[:params.get('limit', 10)]
        
        # 4. æ‰¹é‡è·å–å°é¢URL
        song_ids = [s.id for s in songs]
        covers = SongRecord.objects.filter(
            song_id__in=song_ids
        ).values('song_id', 'cover_url').distinct()
        
        cover_map = {c['song_id']: c['cover_url'] for c in covers}
        
        # 5. ç»„è£…ç»“æœ
        result = []
        for song in songs:
            result.append({
                'id': song.id,
                'song_name': song.song_name,
                'singer': song.singer,
                'perform_count': song.perform_count,
                'cover_url': cover_map.get(song.id),
                'styles': [s.style.name for s in song.song_styles.all()],
                'tags': [t.tag.name for t in song.song_tags.all()]
            })
        
        return QueryResult(
            success=True,
            data=result,
            result_type='song_list'
        )
```

---

## å…«ã€APIè®¾è®¡

### 8.1 LLMé…ç½®API

```python
# POST /api/agent/llm/config

Request:
{
  "provider": "aliyun",  # aliyun, openai, local
  "model": "qwen-max",
  "api_key": "sk-xxx",
  "daily_budget": 50.0
}

Response:
{
  "success": true,
  "message": "LLMé…ç½®å·²æ›´æ–°",
  "config": {
    "provider": "aliyun",
    "model": "qwen-max",
    "daily_budget": 50.0
  }
}
```

### 8.2 æˆæœ¬æŸ¥è¯¢API

```python
# GET /api/agent/llm/cost?date=2026-02-03

Response:
{
  "success": true,
  "date": "2026-02-03",
  "cost": {
    "total_cost": 12.50,
    "total_tokens": 125000,
    "cache_hits": 350,
    "total_requests": 500,
    "cache_hit_rate": 70.0,
    "by_model": {
      "qwen-max": {"cost": 8.00, "requests": 200},
      "qwen-turbo": {"cost": 4.50, "requests": 300}
    },
    "by_task": {
      "intent_recognition": {"cost": 3.00, "requests": 300},
      "response_generation": {"cost": 9.50, "requests": 200}
    }
  }
}
```

### 8.3 å¢å¼ºçš„æŸ¥è¯¢API

```python
# POST /api/agent/query

Request:
{
  "query": "æœ€è¿‘ä¸€ä¸ªæœˆæ¼”å”±çš„å¤é£æ­Œæ›²æœ‰å“ªäº›ï¼Ÿ",
  "user_id": "user_123",
  "use_llm": true,  // æ˜¯å¦ä½¿ç”¨LLM
  "format": "table",
  "enable_explanation": true  // æ˜¯å¦ç”Ÿæˆè§£é‡Š
}

Response:
{
  "success": true,
  "intent": {
    "type": "song_search",
    "parameters": {
      "style": "å¤é£",
      "time_range": "last_month"
    },
    "confidence": 0.96,
    "reasoning": "ç”¨æˆ·è¯¢é—®'æœ€è¿‘ä¸€ä¸ªæœˆ'çš„'å¤é£æ­Œæ›²'ï¼Œè¯†åˆ«ä¸ºæ­Œæ›²æœç´¢å¹¶ç­›é€‰é£æ ¼",
    "recognizer": "llm"  // llm æˆ– rule
  },
  "result": {
    "type": "table",
    "columns": ["æ­Œæ›²åç§°", "æ¼”å”±æ¬¡æ•°", "é¦–æ¬¡æ¼”å”±"],
    "rows": [...],
    "summary": "æœ€è¿‘ä¸€ä¸ªæœˆæ¼”å”±çš„å¤é£æ­Œæ›²"
  },
  "response": {
    "text": "æœ€è¿‘ä¸€ä¸ªæœˆï¼Œå’»å’»æ»¡æ¼”å”±äº†5é¦–å¤é£æ­Œæ›²ï¼Œå…¶ä¸­ã€Šé’èŠ±ç“·ã€‹æ¼”å”±æ¬¡æ•°æœ€å¤šï¼Œå…±3æ¬¡ã€‚ğŸµ",
    "highlights": ["å…±5é¦–å¤é£æ­Œæ›²", "ã€Šé’èŠ±ç“·ã€‹æ¼”å”±3æ¬¡"],
    "suggestions": ["æŸ¥çœ‹æ¯é¦–æ­Œçš„è¯¦ç»†æ¼”å”±è®°å½•", "æœç´¢æ›´å¤šå¤é£æ­Œæ›²"]
  },
  "execution_time": 0.95,
  "from_cache": false,
  "model_used": "qwen-max",
  "cost": 0.08
}
```

---

## ä¹ã€å®‰å…¨ä¸éšç§

### 9.1 æ•æ„Ÿæ•°æ®è¿‡æ»¤

```python
# agent/security/sensitive_data_filter.py

class SensitiveDataFilter:
    """æ•æ„Ÿæ•°æ®è¿‡æ»¤å™¨"""
    
    # æ•æ„Ÿå­—æ®µåˆ—è¡¨
    SENSITIVE_FIELDS = [
        'user_id',
        'ip_address',
        'email',
        'phone',
        'real_name'
    ]
    
    @classmethod
    def filter_query(cls, query: str) -> str:
        """è¿‡æ»¤æŸ¥è¯¢ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        # ç§»é™¤æ½œåœ¨çš„ä¸ªäººä¿¡æ¯
        filtered = re.sub(r'\d{11}', '[æ‰‹æœºå·]', query)  # æ‰‹æœºå·
        filtered = re.sub(r'[\w\.-]+@[\w\.-]+\.\w+', '[é‚®ç®±]', filtered)  # é‚®ç®±
        filtered = re.sub(r'\d{18}', '[èº«ä»½è¯]', filtered)  # èº«ä»½è¯
        
        return filtered
    
    @classmethod
    def filter_result(cls, data: dict) -> dict:
        """è¿‡æ»¤ç»“æœä¸­çš„æ•æ„Ÿå­—æ®µ"""
        filtered = data.copy()
        
        for field in cls.SENSITIVE_FIELDS:
            if field in filtered:
                filtered[field] = '***'
        
        return filtered
```

### 9.2 APIå¯†é’¥ç®¡ç†

```python
# agent/security/api_key_manager.py

import os
from cryptography.fernet import Fernet

class APIKeyManager:
    """APIå¯†é’¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.cipher = Fernet(self._get_or_create_key())
    
    def _get_or_create_key(self) -> bytes:
        """è·å–æˆ–åˆ›å»ºåŠ å¯†å¯†é’¥"""
        key_file = '/tmp/llm_api_key.enc'
        
        if os.path.exists(key_file):
            with open(key_file, 'rb') as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(key_file, 'wb') as f:
                f.write(key)
            os.chmod(key_file, 0o600)  # ä»…æ‰€æœ‰è€…å¯è¯»å†™
            return key
    
    def encrypt_key(self, api_key: str) -> str:
        """åŠ å¯†APIå¯†é’¥"""
        return self.cipher.encrypt(api_key.encode()).decode()
    
    def decrypt_key(self, encrypted_key: str) -> str:
        """è§£å¯†APIå¯†é’¥"""
        return self.cipher.decrypt(encrypted_key.encode()).decode()
    
    def store_key(self, provider: str, api_key: str):
        """å­˜å‚¨APIå¯†é’¥"""
        encrypted = self.encrypt_key(api_key)
        # å­˜å‚¨åˆ°ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
        os.environ[f'LLM_{provider.upper()}_API_KEY'] = encrypted
```

---

## åã€æ€§èƒ½ä¼˜åŒ–

### 10.1 å¼‚æ­¥è°ƒç”¨

```python
# agent/llm/async_llm_client.py

import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncLLMClient:
    """å¼‚æ­¥LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, sync_client):
        self.sync_client = sync_client
        self.executor = ThreadPoolExecutor(max_workers=5)
    
    async def complete_async(self, prompt: str, **kwargs) -> str:
        """å¼‚æ­¥å®Œæˆ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.sync_client.complete,
            prompt,
            **kwargs
        )
    
    async def batch_complete(self, prompts: list) -> list[str]:
        """æ‰¹é‡å®Œæˆ"""
        tasks = [
            self.complete_async(prompt)
            for prompt in prompts
        ]
        return await asyncio.gather(*tasks)
```

### 10.2 Tokenä¼˜åŒ–

```python
# agent/llm/token_optimizer.py

class TokenOptimizer:
    """Tokenä¼˜åŒ–å™¨"""
    
    @staticmethod
    def compress_prompt(prompt: str) -> str:
        """å‹ç¼©æç¤ºè¯"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        compressed = re.sub(r'\s+', ' ', prompt)
        
        # ç§»é™¤ä¸å¿…è¦çš„æ¢è¡Œ
        compressed = compressed.replace('\n', ' ')
        
        # ä½¿ç”¨ç¼©å†™
        abbreviations = {
            "information": "info",
            "description": "desc",
            "parameters": "params"
        }
        
        for full, abbr in abbreviations.items():
            compressed = compressed.replace(full, abbr)
        
        return compressed.strip()
    
    @staticmethod
    def truncate_history(history: list, max_tokens: int = 2000) -> list:
        """æˆªæ–­å†å²è®°å½•"""
        truncated = []
        current_tokens = 0
        
        for msg in reversed(history):
            tokens = len(msg['content']) // 1.5  # ç²—ç•¥ä¼°ç®—
            
            if current_tokens + tokens > max_tokens:
                break
            
            truncated.insert(0, msg)
            current_tokens += tokens
        
        return truncated
```

---

## åä¸€ã€ç›‘æ§ä¸æ—¥å¿—

### 11.1 LLMè°ƒç”¨æ—¥å¿—

```python
# agent/monitoring/llm_logger.py

import logging

logger = logging.getLogger('llm')

class LLMLogger:
    """LLMè°ƒç”¨æ—¥å¿—"""
    
    @staticmethod
    def log_request(model: str, task: str, prompt: str):
        """è®°å½•è¯·æ±‚"""
        logger.info(f"[{task}] Model: {model}, Prompt length: {len(prompt)}")
    
    @staticmethod
    def log_response(model: str, task: str, response: str, 
                     tokens: int, cost: float, latency: float):
        """è®°å½•å“åº”"""
        logger.info(
            f"[{task}] Model: {model}, "
            f"Tokens: {tokens}, "
            f"Cost: Â¥{cost:.4f}, "
            f"Latency: {latency:.2f}s"
        )
    
    @staticmethod
    def log_error(model: str, task: str, error: str):
        """è®°å½•é”™è¯¯"""
        logger.error(f"[{task}] Model: {model}, Error: {error}")
```

### 11.2 æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | ç›‘æ§æ–¹å¼ |
|-----|-------|---------|
| **æ„å›¾è¯†åˆ«å‡†ç¡®ç‡** | >95% | å¯¹æ¯”è§„åˆ™å¼•æ“ç»“æœ |
| **å¹³å‡å“åº”æ—¶é—´** | <1s | APIå“åº”æ—¶é—´ç»Ÿè®¡ |
| **ç¼“å­˜å‘½ä¸­ç‡** | >60% | Redisç¼“å­˜ç»Ÿè®¡ |
| **æ¯æ—¥æˆæœ¬** | <Â¥50 | æˆæœ¬ç›‘æ§ |
| **é”™è¯¯ç‡** | <1% | é”™è¯¯æ—¥å¿—ç»Ÿè®¡ |

---

## åäºŒã€å®æ–½è®¡åˆ’

### 12.1 é˜¶æ®µåˆ’åˆ†

#### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€é›†æˆï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. é›†æˆé€šä¹‰åƒé—®API
2. å®ç°LLMå®¢æˆ·ç«¯å°è£…
3. å®ç°LLMæ„å›¾è¯†åˆ«å™¨
4. å®ç°åŸºç¡€ç¼“å­˜
5. å•å…ƒæµ‹è¯•

**äº¤ä»˜ç‰©**ï¼š
- LLMå®¢æˆ·ç«¯æ¨¡å—
- æ„å›¾è¯†åˆ«å™¨
- æµ‹è¯•ç”¨ä¾‹

#### ç¬¬äºŒé˜¶æ®µï¼šå¢å¼ºåŠŸèƒ½ï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. å®ç°ä¸Šä¸‹æ–‡è§£æå™¨
2. å®ç°ç»“æœç”Ÿæˆå™¨
3. å®ç°æ··åˆæŸ¥è¯¢å¼•æ“
4. æˆæœ¬ç›‘æ§ç³»ç»Ÿ
5. APIæ¥å£å¼€å‘

**äº¤ä»˜ç‰©**ï¼š
- å®Œæ•´çš„LLMé›†æˆæ¨¡å—
- APIæ–‡æ¡£
- æˆæœ¬ç›‘æ§é¢æ¿

#### ç¬¬ä¸‰é˜¶æ®µï¼šä¼˜åŒ–ä¸éƒ¨ç½²ï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. æ€§èƒ½ä¼˜åŒ–ï¼ˆå¼‚æ­¥ã€ç¼“å­˜ï¼‰
2. æˆæœ¬ä¼˜åŒ–ï¼ˆé™çº§ç­–ç•¥ï¼‰
3. å®‰å…¨åŠ å›º
4. ç›‘æ§å‘Šè­¦
5. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

**äº¤ä»˜ç‰©**ï¼š
- æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
- éƒ¨ç½²æ–‡æ¡£
- ç›‘æ§é¢æ¿

### 12.2 é‡Œç¨‹ç¢‘

| é‡Œç¨‹ç¢‘ | æ—¶é—´ | äº¤ä»˜å†…å®¹ |
|-------|------|---------|
| M1 | ç¬¬1å‘¨ | LLMåŸºç¡€é›†æˆå®Œæˆ |
| M2 | ç¬¬2å‘¨ | å¢å¼ºåŠŸèƒ½å®Œæˆ |
| M3 | ç¬¬3å‘¨ | ç”Ÿäº§ç¯å¢ƒä¸Šçº¿ |

---

## åä¸‰ã€é£é™©è¯„ä¼°ä¸åº”å¯¹

### 13.1 é£é™©æ¸…å•

| é£é™© | å½±å“ | æ¦‚ç‡ | åº”å¯¹æªæ–½ |
|-----|-----|-----|---------|
| **APIæˆæœ¬è¶…æ”¯** | é«˜ | ä¸­ | é¢„ç®—æ§åˆ¶ã€ç¼“å­˜ã€é™çº§åˆ°è§„åˆ™å¼•æ“ |
| **APIä¸ç¨³å®š** | ä¸­ | ä¸­ | å¤šæä¾›å•†æ”¯æŒã€è‡ªåŠ¨é‡è¯•ã€é™çº§ |
| **å“åº”æ—¶é—´è¿‡é•¿** | ä¸­ | ä½ | å¼‚æ­¥è°ƒç”¨ã€ç¼“å­˜ã€æœ¬åœ°æ¨¡å‹é™çº§ |
| **æ•°æ®æ³„éœ²** | é«˜ | ä½ | æ•æ„Ÿæ•°æ®è¿‡æ»¤ã€åŠ å¯†å­˜å‚¨ |
| **è¯†åˆ«å‡†ç¡®ç‡ä¸‹é™** | ä¸­ | ä½ | æ··åˆå¼•æ“ã€äººå·¥å®¡æ ¸ |

### 13.2 é™çº§ç­–ç•¥

```python
# agent/fallback/fallback_manager.py

class FallbackManager:
    """é™çº§ç®¡ç†å™¨"""
    
    LEVELS = [
        {
            'name': 'llm',
            'confidence_threshold': 0.95,
            'use_llm': True
        },
        {
            'name': 'hybrid',
            'confidence_threshold': 0.70,
            'use_llm': True,
            'fallback_to_rules': True
        },
        {
            'name': 'rules',
            'confidence_threshold': 0.0,
            'use_llm': False
        }
    ]
    
    @classmethod
    def get_level(cls, cost_budget: float, api_status: bool) -> dict:
        """è·å–å½“å‰é™çº§çº§åˆ«"""
        # APIä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™å¼•æ“
        if not api_status:
            return cls.LEVELS[2]
        
        # é¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨æ··åˆå¼•æ“
        if cost_budget < 10.0:
            return cls.LEVELS[1]
        
        # æ­£å¸¸æƒ…å†µï¼Œä½¿ç”¨LLM
        return cls.LEVELS[0]
```

---

## åå››ã€æ€»ç»“

### 14.1 æ–¹æ¡ˆä¼˜åŠ¿

1. **æ¸è¿›å¼æ¥å…¥**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œé™ä½é£é™©
2. **æˆæœ¬å¯æ§**ï¼šå¤šå±‚ç¼“å­˜ã€é¢„ç®—æ§åˆ¶ã€é™çº§ç­–ç•¥
3. **é«˜å¯ç”¨æ€§**ï¼šå¤šæä¾›å•†æ”¯æŒã€è‡ªåŠ¨é™çº§
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¼‚æ­¥è°ƒç”¨ã€Tokenä¼˜åŒ–ã€æ‰¹é‡å¤„ç†
5. **å¯è§‚æµ‹æ€§**ï¼šå®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

### 14.2 é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | æ¥å…¥å‰ | æ¥å…¥å | æå‡ |
|-----|-------|-------|-----|
| **æ„å›¾è¯†åˆ«å‡†ç¡®ç‡** | 85% | 95%+ | +12% |
| **æ”¯æŒæŸ¥è¯¢å¤æ‚åº¦** | ç®€å• | å¤æ‚ | è´¨çš„é£è·ƒ |
| **å¯¹è¯è‡ªç„¶åº¦** | æœºæ¢° | æµç•… | æ˜¾è‘—æå‡ |
| **ç”¨æˆ·æ»¡æ„åº¦** | 70% | 90%+ | +20% |
| **æ—¥å‡æˆæœ¬** | Â¥0 | Â¥30-50 | æ–°å¢æˆæœ¬ |

### 14.3 åç»­ä¼˜åŒ–æ–¹å‘

1. **Fine-tuning**ï¼šé’ˆå¯¹éŸ³ä¹é¢†åŸŸå¾®è°ƒæ¨¡å‹
2. **æœ¬åœ°éƒ¨ç½²**ï¼šéƒ¨ç½²æœ¬åœ°æ¨¡å‹é™ä½æˆæœ¬
3. **å¤šæ¨¡æ€**ï¼šæ”¯æŒå›¾ç‰‡ã€éŸ³é¢‘æŸ¥è¯¢
4. **ä¸ªæ€§åŒ–**ï¼šåŸºäºç”¨æˆ·åå¥½ä¼˜åŒ–Prompt
5. **è”é‚¦å­¦ä¹ **ï¼šä¿æŠ¤éšç§çš„æ•°æ®å¢å¼º

---

## é™„å½•

### A. ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env

# é€šä¹‰åƒé—®é…ç½®
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxx

# OpenAIé…ç½®ï¼ˆå¯é€‰ï¼‰
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx

# æˆæœ¬æ§åˆ¶
LLM_DAILY_BUDGET=50.0
LLM_CACHE_ENABLED=true

# é™çº§ç­–ç•¥
LLM_FALLBACK_TO_RULES=true
LLM_MULTI_PROVIDER=true
```

### B. é…ç½®ç¤ºä¾‹

```python
# agent/config/llm_config.py

LLM_CONFIG = {
    "default_provider": "aliyun",
    "providers": {
        "aliyun": {
            "model": "qwen-max",
            "api_key": os.getenv("DASHSCOPE_API_KEY"),
            "timeout": 30,
            "max_retries": 3
        },
        "openai": {
            "model": "gpt-4o-mini",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "timeout": 30,
            "max_retries": 3
        }
    },
    "cost_control": {
        "daily_budget": 50.0,
        "alert_threshold": 40.0
    },
    "cache": {
        "enabled": True,
        "timeout": {
            "intent_recognition": 86400,
            "response_generation": 3600
        }
    }
}
```

### C. æµ‹è¯•ç”¨ä¾‹

```python
# agent/tests/test_llm_integration.py

import pytest
from agent.intent_recognizer.llm_recognizer import LLMIntentRecognizer

class TestLLMIntentRecognition:
    
    @pytest.fixture
    def recognizer(self):
        return LLMIntentRecognizer()
    
    def test_song_search(self, recognizer):
        """æµ‹è¯•æ­Œæ›²æœç´¢è¯†åˆ«"""
        query = "æœç´¢ã€Šç¨»é¦™ã€‹"
        intent = recognizer.recognize(query)
        
        assert intent.type == IntentType.SONG_SEARCH
        assert intent.parameters['song_name'] == 'ç¨»é¦™'
        assert intent.confidence > 0.9
    
    def test_complex_query(self, recognizer):
        """æµ‹è¯•å¤æ‚æŸ¥è¯¢è¯†åˆ«"""
        query = "2024å¹´æ¼”å”±æœ€å¤šçš„5é¦–å¤é£æ­Œæ›²"
        intent = recognizer.recognize(query)
        
        assert intent.type == IntentType.SONG_STATS
        assert intent.parameters['year'] == 2024
        assert intent.parameters['limit'] == 5
        assert intent.parameters.get('style') == 'å¤é£'
    
    def test_context_aware(self, recognizer):
        """æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥"""
        context = {
            "history": [
                {"role": "user", "content": "æœ€è¿‘æ¼”å”±çš„æ­Œæ›²"},
                {"role": "assistant", "content": "ã€Šç¨»é¦™ã€‹ã€ã€Šé’èŠ±ç“·ã€‹..."}
            ],
            "entities": ["ç¨»é¦™", "é’èŠ±ç“·"]
        }
        query = "è¿™é¦–æ­Œç¬¬ä¸€æ¬¡å”±æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ"
        
        intent = recognizer.recognize(query, context)
        assert intent.parameters.get('song_name') in context['entities']
```