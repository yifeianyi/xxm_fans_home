# B站投稿数据爬虫 - 实现总结

## 1. 项目概述

本项目实现了B站投稿数据的定时爬取系统，用于追踪和记录B站视频的播放量、弹幕数、评论数、点赞数、投币数、收藏数、转发数等关键指标。

### 核心功能
- 从 Django 数据库导出作品列表
- 定时爬取B站视频数据（每小时执行）
- 数据去重（相同BV号只保留第一个）
- 数据持久化到SQLite数据库
- 完善的日志记录和异常处理

---

## 2. 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                     B站投稿数据爬虫系统                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   spider/                        tools/spider/              │
│   ├── run_views_crawler.py  ───▶├── export_views.py         │
│   └── README.md                 ├── crawl_views.py          │
│                                 ├── import_views.py         │
│                                 └── utils/                  │
│                                     ├── logger.py           │
│                                     └── __init__.py         │
│                                                             │
│   data/                                                     │
│   ├── spider/views.json          # 导出的作品列表            │
│   ├── spider/views/              # 按小时存储的爬取结果       │
│   └── view_data.sqlite3          # SQLite数据库              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 3. 核心模块

### 3.1 主控脚本 (spider/run_views_crawler.py)
**职责**：统一的命令行入口，整合所有功能

**支持参数**：
```bash
python spider/run_views_crawler.py --full          # 完整流程（默认）
python spider/run_views_crawler.py --export-only   # 仅导出
python spider/run_views_crawler.py --crawl-only    # 仅爬取
python spider/run_views_crawler.py --import-only   # 仅导入
```

### 3.2 导出模块 (tools/spider/export_views.py)
**职责**：将 `WorkStatic` 表数据导出为 `views.json`

**触发方式**：
- 手动执行
- 信号自动触发（WorkStatic表变更时自动导出）

**输出格式**：
```json
{
  "export_time": "2026-02-06T12:00:00",
  "total_count": 223,
  "valid_count": 222,
  "works": [...]
}
```

### 3.3 爬虫模块 (tools/spider/crawl_views.py)
**职责**：核心爬取逻辑

**关键技术**：
- 强化版API客户端 (`RobustBilibiliAPIClient`)
- 细粒度超时控制（连接3秒，读取5秒）
- 随机请求间隔（1-3秒）
- BV号去重（相同BV号只爬取第一个）
- 完善的异常分类处理

**爬取字段**：
- view_count（播放数）
- danmaku_count（弹幕数）
- comment_count（评论数）
- like_count（点赞数）
- coin_count（投币数）
- favorite_count（收藏数）
- share_count（转发数）

**输出文件命名**：
```
data/spider/views/{year}/{month}/{day}/{date}-{hour}_views_data.json
```

### 3.4 导入模块 (tools/spider/import_views.py)
**职责**：将爬取结果导入SQLite数据库

**数据表结构**：
```sql
-- 作品数据表
data_analytics_workmetricsspider
  - platform, work_id, title
  - crawl_date, crawl_hour, crawl_time
  - view_count, danmaku_count, comment_count
  - like_count, coin_count, favorite_count, share_count

-- 会话表
data_analytics_crawlsessionspider
  - session_id, crawl_date, crawl_hour
  - total_count, success_count, fail_count
```

---

## 4. 技术特点

### 4.1 异常处理机制
- **连接超时**：3秒
- **读取超时**：5秒
- **重试机制**：最多2次
- **错误分类**：ConnectTimeout、ReadTimeout、ConnectionError、APIError

### 4.2 防反爬策略
- 随机请求间隔：1-3秒
- 请求头伪装（User-Agent、Referer）
- 快速失败（减少重试次数）
- 详细的请求耗时日志

### 4.3 数据一致性
- 事务化数据库操作
- 幂等导入（INSERT OR REPLACE）
- 去重处理（相同BV号只保留第一个）

---

## 5. 部署与使用

### 5.1 目录结构
```
spider/
├── run_views_crawler.py      # 主控脚本
└── README.md                 # 使用说明

repo/xxm_fans_backend/tools/spider/
├── __init__.py
├── export_views.py           # 导出模块
├── crawl_views.py            # 爬虫模块
├── import_views.py           # 导入模块
└── utils/
    ├── __init__.py
    └── logger.py             # 日志工具

infra/systemd/
├── bilibili-views-crawler.service  # Systemd服务
└── bilibili-views-crawler.timer    # Systemd定时器
```

### 5.2 手动执行
```bash
# 完整流程
python spider/run_views_crawler.py

# 仅导出
python spider/run_views_crawler.py --export-only

# 仅爬取
python spider/run_views_crawler.py --crawl-only

# 仅导入
python spider/run_views_crawler.py --import-only --date 2026-02-06 --hour 12
```

### 5.3 定时任务
```bash
# 激活定时任务
sudo systemctl enable --now infra/systemd/bilibili-views-crawler.timer

# 查看状态
systemctl list-timers --all | grep bilibili
```

### 5.4 日志查看
```bash
# 爬虫日志
tail -f logs/spider/crawl_views_YYYYMMDD.log

# 导入日志
tail -f logs/spider/import_views_YYYYMMDD.log

# 主控日志
tail -f logs/spider/run_views_crawler_YYYYMMDD.log
```

---

## 6. 数据库配置

### 6.1 数据库路由
```python
DATABASES = {
    'default': {...},
    'view_data_db': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'data' / 'view_data.sqlite3',
    },
}
```

### 6.2 表结构
```
data/view_data.sqlite3
├── data_analytics_account          # 账号数据
├── data_analytics_followermetrics  # 粉丝数据
├── data_analytics_workstatic       # 作品静态数据
├── data_analytics_workmetricsspider    # 爬虫数据
└── data_analytics_crawlsessionspider   # 爬虫会话
```

---

## 7. 信号自动触发

当 `WorkStatic` 表数据变更时，自动触发 `views.json` 导出：

```python
@receiver(post_save, sender=WorkStatic)
def auto_export_views_on_save(sender, instance, created, **kwargs):
    # 防抖机制：10秒内只触发一次
    # 延迟5秒执行，合并短时间内的多次变更
```

---

## 8. 注意事项

1. **请求间隔**：默认1-3秒随机间隔，避免触发反爬
2. **超时设置**：连接3秒，读取5秒，防止长时间卡住
3. **重试次数**：最多2次，快速失败
4. **数据去重**：相同BV号只保留第一个（按数据库顺序）
5. **数据存储**：每小时一个JSON文件 + SQLite数据库

---

## 9. 后续优化建议

1. **Web管理界面**：开发简单的Web界面查看统计数据
2. **数据可视化**：使用图表展示播放量、点赞数趋势
3. **异常告警**：爬取失败时发送通知（钉钉/邮件）
4. **分布式爬取**：支持多节点部署，提高爬取效率
5. **数据备份**：定期备份SQLite数据库到云端存储

---

## 10. 相关文档

- `doc/spider/B站投稿数据爬虫-需求.md` - 原始需求文档
- `doc/spider/B站投稿数据爬虫-实现方案.md` - 详细实现方案
- `spider/README.md` - 使用说明

---

**创建时间**：2026-02-06  
**作者**：AI Assistant  
**版本**：v1.0
